---
title: "RSY: MC1"
subtitle: "Studiengang Data Science (HS2022), FHNW"
author: "Jan Zwicky und Gabriel Torres"
date: "Letzte Aktualisierungen: `r format(Sys.time(), '%B %d, %Y')`"
output:
  bookdown::html_document2:
      code_folding: show
      toc: true
      toc_depth: 2
      toc_float: true
      number_sections: true
editor_options: 
  chunk_output_type: console
---
<style>
#TOC {
  background-color: #F5F5F5;
  font-size: 16px;
}
#header{
  color: #708090;
  background-color: #F5F5F5;
  font-size: 30px;
}
body{
  color: #708090;
  background-color:#F5F5F5;
}
</style>


# Pakete laden und Daten einlesen {-}
## Pakete laden {-}

```{r setup, cache = TRUE, message = FALSE, warning = FALSE}
# Pakete für Data Wrangling und Visualisierung
library(tidyverse)
library(rsample)
library(hablar)
library(gridExtra)
library(fastDummies)

# Pakete für das HTML
library(bookdown)
library(knitr)

# Recommenderlab und ähnlich
library(recommenderlab)
library(vegan)
library(coop)
library(stats)
```
##  Konfiguration {-}
```{r}
# Konfiguration der Pakete
knitr::opts_chunk$set(fit.align = "left", cache = TRUE, warning = FALSE, message = FALSE)
```

## Daten einlesen {-}
```{r}
# Einlesen der CSV-Dateien und erstellen der samples
movies1 <- read.csv("ml-latest-small/movies.csv", sep = ",")
ratings1 <- read.csv("ml-latest-small/ratings.csv", sep = ",")

# Sample von 70%
set.seed(69)
movies2 <- movies1 %>% slice_sample(prop = 0.7)
ratings2 <- subset(ratings1, movieId %in% movies2$movieId) %>% slice_sample(prop = 0.7)

# 2ter Sample von 70%
set.seed(100)
movies1 <- movies1 %>% slice_sample(prop = 0.7)
ratings1 <- subset(ratings1, movieId %in% movies1$movieId) %>% slice_sample(prop = 0.7)
```

# EDA
## Welches sind die am häufigsten geschauten Filme?
### Sample 1
```{r}
kable(left_join(movies1, ratings1, "movieId") %>%
  group_by(title, movieId, genres) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  head(3))
```

### Sample 2
```{r}
kable(left_join(movies2, ratings2, "movieId") %>%
  group_by(title, movieId, genres) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  head(3))
```

### Beschreibung
In den beiden Outputs haben wir die Aufzählung der 3 meist bewerteten Filme, bei dem die Spalte 'count' die Anzahl Bewertungen ist.
Die Top 3 Filme wurden bei beiden Datensätzen etwa 180 bis 240 mal bewertet. Pulp Fiction und Star Wars kommen in beiden Datensätzen in unterschiedlicher Anzahl vor.
Wir können nicht bestimmen, wie oft ein Film geschaut wurde, da es zu dieser Information keine Daten gibt. Als alternative definieren wir, dass geschaut und bewertet gleichgestellt wird.

## Welches sind die am häufigsten geschauten Genres?
### Sample 1
```{r}
genres_sep1 <- movies1 %>%
  separate_rows(genres, sep = "\\|", convert = FALSE) %>%
  replace(. == "", "no genres listed")

kable(genres_sep1 %>%
  right_join(ratings1, "movieId") %>%
  group_by(genres) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  head(3))
```

### Sample 2
```{r}
genres_sep2 <- movies2 %>%
  separate_rows(genres, sep = "\\|", convert = FALSE) %>%
  replace(. == "", "no genres listed")

kable(genres_sep2 %>%
  right_join(ratings2, "movieId") %>%
  group_by(genres) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  head(3))
```

### Beschreibung
In beiden Outputs haben wir die meist bewerteten Filmgenres, bei dem die Spalte 'count' signalisiert, bei wie vielen Filmbewertungen der bewertete Film dieses Genre beinhaltet. Der Outputs ist bei beiden Datensätzen sehr ähnlich.
Die am meist geschauten/bewerteten Genres sind Drama, Comedy und Action.

## Wie verteilen sich die Kundenratings gesamthaft?
### Sample 1

```{r}
ggplot(ratings1, aes(rating)) +
  geom_bar() +
  labs(
    title = "Verteilung der Kundenratings",
    x = "Bewertung",
    y = "Anzahl Bewertungen",
    subtitle = paste("Durchschnittsbewertung: ", round(mean(ratings1$rating), 2))
  ) +
  theme_classic() +
  theme(legend.position = "none")
```

### Sample 2
```{r}
ggplot(ratings2, aes(rating)) +
  geom_bar() +
  labs(
    title = "Verteilung der Kundenratings",
    x = "Bewertung",
    y = "Anzahl Bewertungen",
    subtitle = paste("Durchschnittsbewertung: ", round(mean(ratings2$rating), 2))
  ) +
  theme_classic() +
  theme(legend.position = "none")
```

### Beschreibung
In diesen Plots wird die Verteilung der Kundenratings visualisiert. 
Die meisten Bewertungen sind im Bereich der natürlichen Zahlen, wenige Bewertungen sind ein Wert zwischen zwei dieser Zahlen. Öfters enthält eine Bewertung den Wert 4. Der Durchschnitt aller Bewertungen liegt bei etwa 3,5.

## Wie verteilen sich die Kundenratings nach Genres?
### Sample 1
```{r}
# Nach Genres
genres_sep_ratings1 <- genres_sep1 %>%
  right_join(ratings1, "movieId")
ggplot(genres_sep_ratings1, aes(x = rating, fill = genres)) +
  geom_bar(aes(y = ..prop.., group = 1)) +
  facet_wrap(~genres) +
  labs(
    title = "Verteilung der Kundenratings nach Genre",
    x = "Bewertung",
    y = "Verteilung",
  ) +
  theme_classic() +
  theme(legend.position = "none")
```

### Sample 2
```{r}
# Nach Genres
genres_sep_ratings2 <- genres_sep2 %>%
  right_join(ratings2, "movieId")
ggplot(genres_sep_ratings2, aes(x = rating, fill = genres)) +
  geom_bar(aes(y = ..prop.., group = 1)) +
  facet_wrap(~genres) +
  labs(
    title = "Verteilung der Kundenratings nach Genre",
    x = "Bewertung",
    y = "Verteilung",
  ) +
  theme_classic() +
  theme(legend.position = "none")
```

### Beschreibung:
Es wird die Verteilung der Kundenratings **nach Kategorie** visualisiert.
Die Verteilung der Kundenratings ähneln sich bei vielen Kategorien der Verteilung der Gesamtmenge. Jedoch mit einigen Ausnahmen: Dokumentarfilme haben zum Beispiel überdurchschnittlich viele Bewertungen mit dem Wert 4 und unterdurchschnittlich wenig Bewertungen mit dem Wert 3 und 5. Man könnte sagen, dass Dokumentarfilme sehr konstante Ratings haben.

## Wie verteilen sich die mittleren Kundenratings pro Film?
### Alle Daten Sample 1
```{r}
mean_rating_movie1 <- ratings1 %>%
  group_by(movieId) %>%
  summarise(mean_rating = mean(rating), count = n())

ggplot(mean_rating_movie1, aes(mean_rating)) +
  geom_histogram(bins = 50) +
  labs(
    title = "Verteilung der mittleren Kundenratings pro Film",
    x = "Durchschnittliche Bewertung",
    y = "Verteilung"
  ) +
  theme_classic()
```

### Alle Daten Sample 2
```{r}
mean_rating_movie2 <- ratings2 %>%
  group_by(movieId) %>%
  summarise(mean_rating = mean(rating), count = n())

ggplot(mean_rating_movie2, aes(mean_rating)) +
  geom_histogram(bins = 50) +
  labs(
    title = "Verteilung der mittleren Kundenratings pro Film",
    x = "Durchschnittliche Bewertung",
    y = "Verteilung"
  ) +
  theme_classic()
```

### Beschreibung:
Hier wird die Verteilung der Durchschnittswerte der Bewertungen nach Film visualisiert.
Da einige Filme nur wenige Bewertungen haben, liegen sehr viele Mittelwerte bei ganzen oder halben Zahlen. Deswegen gibt es bei unseren Plots einige hohe Balken.

### Filme mit mehr als 5 Bewertungen Sample 1
```{r}
ggplot(mean_rating_movie1 %>% filter(count >= 5), aes(mean_rating)) +
  geom_histogram(bins = 50) +
  labs(
    title = "Verteilung der mittleren Kundenratings pro Film",
    subtitle = "Filme mit mehr als 5 Bewertungen",
    x = "Durchschnittliche Bewertung",
    y = "Verteilung"
  ) +
  theme_classic()
```

### Filme mit mehr als 5 Bewertungen Sample 2
```{r}
ggplot(mean_rating_movie2 %>% filter(count >= 5), aes(mean_rating)) +
  geom_histogram(bins = 50) +
  labs(
    title = "Verteilung der mittleren Kundenratings pro Film",
    subtitle = "Filme mit mehr als 5 Bewertungen",
    x = "Durchschnittliche Bewertung",
    y = "Verteilung"
  ) +
  theme_classic()
```

### Beschreibung:
Hier wird das gleiche wie beim letzten Plot visualisiert. Jedoch wurden Filme mit weniger als 5 Bewertungen entfernt.
Wenn man alle Filme mit weniger als 5 Bewertungen entfernt, erkennt man, dass die Bewertungen der Filme linksschief verteilt sind.

## Wie viele Filme, wurden je nach durchschnittlichem Kundenrating geratet.
### Sample 1
```{r}
ggplot(mean_rating_movie1, aes(mean_rating, count, color = mean_rating)) +
  geom_point(alpha = 0.3) +
  labs(
    title = "Anzahl Bewertungen der mittleren Kundenratings pro Film",
    x = "Durchschnittliche Bewertung",
    y = "Anzahl Bewertungen"
  ) +
  theme_classic() +
  scale_color_gradient(low = "red", high = "green") +
  theme(legend.position = "none")
```

### Sample 2
```{r}
ggplot(mean_rating_movie2, aes(mean_rating, count, color = mean_rating)) +
  geom_point(alpha = 0.3) +
  labs(
    title = "Anzahl Bewertungen der mittleren Kundenratings pro Film",
    x = "Durchschnittliche Bewertung",
    y = "Anzahl Bewertungen"
  ) +
  theme_classic() +
  scale_color_gradient(low = "red", high = "green") +
  theme(legend.position = "none")
```

### Beschreibung:
Hier werden die Durchschnittswerte der Bewertungen nach Film in Abhängigkeit von der Anzahl Bewertungen nach Film visualisiert.
Hier werden die gleichen Daten anders dargestellt. Man erkennt, dass desto öfters ein Film bewertet wird, desto näher liegt die durchschnittliche Bewertung bei 4. Man kann dies vielleicht begründen, indem man sagt, dass ein schlechter Film weniger geschaut und deswegen weniger bewertet wird. Jedoch können wir uns nur schwer erklären, wieso Filme mit einer Bewertung über 4 nicht so oft geschaut/bewertet werden. 

## Wie stark streuen die Ratings von individuellen Kunden?
### Sample 1

```{r}
set.seed(100)
sample_values <- sample(1:610, 4, replace = FALSE)

ratings1 %>%
  filter(userId %in% sample_values) %>%
  group_by(userId, rating) %>%
  summarise(number = n()) %>%
  group_by(userId) %>%
  mutate(perc = number / sum(number)) %>%
  ungroup() %>%
  ggplot(., aes(rating, perc, fill = factor(userId))) +
  geom_bar(stat = "identity", position = position_dodge(preserve = "single")) +
  labs(
    title = "Streuung von Bewertungen von Kunden",
    subtitle = "random sample",
    x = "Bewertung",
    y = "Verteilung",
    fill = "User ID"
  ) +
  theme_classic()
```

### Sample 2

```{r}
set.seed(101)
sample_values <- sample(1:610, 4, replace = FALSE)

ratings2 %>%
  filter(userId %in% sample_values) %>%
  group_by(userId, rating) %>%
  summarise(number = n()) %>%
  group_by(userId) %>%
  mutate(perc = number / sum(number)) %>%
  ungroup() %>%
  ggplot(., aes(rating, perc, fill = factor(userId))) +
  geom_bar(stat = "identity", position = position_dodge(preserve = "single")) +
  labs(
    title = "Streuung von Bewertungen von Kunden",
    subtitle = "random sample",
    x = "Bewertung",
    y = "Verteilung",
    fill = "User ID"
  ) +
  theme_classic()
```

**Beschreibung:**
Die meisten Kunden haben tendenziell hohe Bewertungen. Es gibt Kunden die keine tiefe Bewertungen abgeben. Solche Kunden die tiefe Bewertungen abgeben (z.B. User ID 442), varieren bei ihren tiefen Bewertungen.
Auch bei den anderen Bewertungen, varieren die Kunden stark.

**Interpretation:**
Es gibt diverse Kunden, die nur Filme bewerten, welche sie auch mögen.

## Wie stark streuen die Ratings von allen Kunden
### Sample 1
```{r}
sd_ratings1 <- ratings1 %>%
  group_by(userId) %>%
  summarise(SD = sd(rating), count = n())


ggplot(sd_ratings1, aes(SD)) +
  geom_boxplot() +
  labs(
    title = "Standardabweichung der Ratings pro User",
    x = "Standardabweichung",
    subtitle = paste("Durchschnittsstandardabweichung: ", round(mean(sd_ratings1$SD), 3)),
  ) +
  theme_classic()
```


### Sample 2
```{r}
sd_ratings2 <- ratings2 %>%
  group_by(userId) %>%
  summarise(SD = sd(rating), count = n())

ggplot(sd_ratings2, aes(SD)) +
  geom_boxplot() +
  labs(
    title = "Standardabweichung der Ratings pro User",
    x = "Standardabweichung",
    subtitle = paste("Durchschnittsstandardabweichung: ", round(mean(sd_ratings2$SD), 3)),
  ) +
  theme_classic()
```

**Beschreibung:**
Die Standardabweichungen der Bewertungen der User werden visualisiert. Dabei erkennt man, dass es eine Observation bei 0 gibt. Die meisten Observationen liegen zwischen 0.7 und 1.1

**Schlussfolgerung:**
Nur ein User hat alles gleiche Bewertungen. Wenn dieser User für einen Recommender mit standardisierten Daten verwendet wird ist es nicht mehr möglich eine Vorhersage zu machen.

Ansonsten ist der Median der Standardabweichungen leicht unter 1. Dies macht bei leicht linksschiefen Daten Sinn.

## Welchen Einfluss hat die Normierung der Ratings pro Kunde auf deren Verteilung?
### Sample 1
```{r}
norm_ratings1 <- ratings1 %>%
  group_by(userId) %>%
  summarise(mean_rating = mean(rating), sd_rating = sd(rating)) %>%
  full_join(., ratings1, by = "userId")

norm_ratings1$z_rating <- (norm_ratings1$rating - norm_ratings1$mean_rating) /
  norm_ratings1$sd_rating

ggplot(norm_ratings1, aes(z_rating)) +
  geom_density() +
  labs(
    title = "Normierte Ratings",
    x = "Z-Normiertes Rating",
    y = "Verteilung"
  ) +
  theme_classic()
```


### Sample 2
```{r}
norm_ratings2 <- ratings2 %>%
  group_by(userId) %>%
  summarise(mean_rating = mean(rating), sd_rating = sd(rating)) %>%
  full_join(., ratings2, by = "userId")

norm_ratings2$z_rating <- (norm_ratings2$rating - norm_ratings2$mean_rating) /
  norm_ratings2$sd_rating

ggplot(norm_ratings2, aes(z_rating)) +
  geom_density() +
  labs(
    title = "Normierte Ratings",
    x = "Z-Normiertes Rating",
    y = "Verteilung"
  ) +
  theme_classic()
```


**Beschreibung:**
Der Mittelwert der Bewertungen pro User befindet sich jetzt bei 0. Es hat nun gleich viele Ratings über bzw. unter 0.

**Schlussfolgerung:**
Alle Ratings unter 0 könnte man als "gefällt dem User nicht" interpretieren und alle Rating über 0 könnte man als "gefällt dem User" interpretieren. Desto weiter sich die Bewertung von 0 entfernt desto mehr oder weniger gefällt dem User der Film.

## Welche strukturellen Charakteristika (z.B.Sparsity) und Auffälligkeiten zeigt die User-Item Matrix?
### Sample 1
```{r}
user_item1 <- norm_ratings1 %>%
  select(movieId, userId, z_rating) %>%
  pivot_wider(names_from = movieId, values_from = z_rating)

sparsity1 <- round(sum(is.na(user_item1)) / (dim(user_item1)[1] * (dim(user_item1)[2])), 3)
```

### Sample 2
```{r}
user_item2 <- norm_ratings2 %>%
  select(movieId, userId, z_rating) %>%
  pivot_wider(names_from = movieId, values_from = z_rating)

sparsity2 <- round(sum(is.na(user_item2)) / (dim(user_item2)[1] * (dim(user_item2)[2])), 3)
```

**Beschreibung:**
Die Sparsity beschreibt, wie hoch der Anteil der NA in den Daten ist. Im Datensatz 1 ist dieser Anteil `r sparsity1 * 100` % . Im Datensatz 2 ist die Sparsity `r sparsity2 *100` %.

**Schlussfolgerung:**
Beide User-Item Matrizen sind etwa zu `r round(mean(c(sparsity1,sparsity2)),3) * 100` % Sparse. Dies bedeutet, dass ein grosser Teil der Daten mit NA's besetzt ist. Für spätere Berechnungen ist es entscheidend, was mit diesen NA's gemacht wird.

# Datenreduktion
Die Daten wurden auf 400 Kunden und 700 Filme reduziert, indem Filme und Kunden mit sehr wenigen Ratings entfernt wurden.

## Reduktion
### Sample 1
```{r}
# Filter 700 most rated movies
top_n_movies1 <- norm_ratings1 %>%
  group_by(movieId) %>%
  count() %>%
  arrange(desc(n)) %>%
  head(700)

# Join data on 700 most rated movies
user_item_r1 <-
  left_join(
    top_n_movies1,
    norm_ratings1,
    by = "movieId"
  )

# Filter 700 most rated user
top_n_user1 <- user_item_r1 %>%
  group_by(userId) %>%
  count() %>%
  arrange(desc(n)) %>%
  head(400) %>%
  ungroup()

# Join data on 400 most rated user (only 700 movies)
user_item_r1 <-
  left_join(
    top_n_user1,
    user_item_r1,
    by = "userId"
  ) %>%
  select(userId, movieId, z_rating)

# Pivot wider
m_user_item_r1 <- user_item_r1 %>%
  pivot_wider(names_from = movieId, values_from = z_rating) %>%
  column_to_rownames(., var = "userId")
```



### Sample 2
```{r}
# Filter 700 most rated movies
top_n_movies2 <- norm_ratings2 %>%
  group_by(movieId) %>%
  count() %>%
  arrange(desc(n)) %>%
  head(700)

# Join data on 700 most rated movies
user_item_r2 <-
  left_join(
    top_n_movies2,
    norm_ratings2,
    by = "movieId"
  )

# Filter 700 most rated user
top_n_user2 <- user_item_r2 %>%
  group_by(userId) %>%
  count() %>%
  arrange(desc(n)) %>%
  head(400) %>%
  ungroup()

# Join data on 400 most rated user (only 700 movies)
user_item_r2 <-
  left_join(
    top_n_user2,
    user_item_r2,
    by = "userId"
  ) %>%
  select(userId, movieId, z_rating)

# Pivot wider
m_user_item_r2 <- user_item_r2 %>%
  pivot_wider(names_from = movieId, values_from = z_rating) %>%
  column_to_rownames(., var = "userId")
```

## Sparsity nach Datenreduktion
### Sample 1
```{r}
# Sparsity Sample 1
sparsity_after_red1 <- sum(is.na(m_user_item_r1)) / (dim(m_user_item_r1)[1] * (dim(m_user_item_r1)[2]))
```

### Sample 2
```{r}
# Sparsity Sample 2
sparsity_after_red2 <- sum(is.na(m_user_item_r2)) / (dim(m_user_item_r2)[1] * (dim(m_user_item_r2)[2]))
```

**Beschreibung:**
Die Sparsities betragen `r round(sparsity_after_red1,3) * 100` % und `r round(sparsity_after_red2,3) * 100` %.

**Schlussfolgerung:**
Somit wurden die Sparsities zum kompletten Datensatz deutlich reduziert. Das bedeutet, dass die User-Item Matrix im Verhältniss mehr Ratings beinhalten.

## Mittlere Kundenratings pro Film vor und nach Datenreduktion
### Sample 1

**Kompletter Datensatz**
```{r}
moviemeans1 <- colMeans(user_item1 %>% column_to_rownames(., var = "userId"), na.rm = TRUE)
moviemeans1 <- data.frame(moviemeans1)
ggplot(moviemeans1, aes(moviemeans1)) +
  geom_density() +
  labs(
    title = "Streuung von durchschnittlichen Bewertung von Filmen",
    subtitle = "kompletter Datensatz 1",
    x = "durchschnittliche Bewertung",
    y = "Verteilung"
  ) +
  theme_classic() +
  xlim(-2, 2)
```

**Reduzierter Datensatz**
```{r}
moviemeans_reducted1 <- colMeans(m_user_item_r1, na.rm = TRUE)
moviemeans_reducted1 <- data.frame(moviemeans_reducted1)
ggplot(moviemeans_reducted1, aes(moviemeans_reducted1)) +
  geom_density() +
  labs(
    title = "Streuung von durchschnittlichen Bewertung von Filmen",
    subtitle = "reduzierter Datensatz 1",
    x = "durchschnittliche Bewertung",
    y = "Verteilung"
  ) +
  theme_classic() +
  xlim(-2, 2)
```

### Sample 2

**Kompletter Datensatz**
```{r}
moviemeans2 <- colMeans(user_item2 %>% column_to_rownames(., var = "userId"), na.rm = TRUE)
moviemeans2 <- data.frame(moviemeans2)
ggplot(moviemeans2, aes(moviemeans2)) +
  geom_density() +
  labs(
    title = "Streuung von durchschnittlichen Bewertung von Filmen",
    subtitle = "kompletter Datensatz 2",
    x = "durchschnittliche Bewertung",
    y = "Verteilung"
  ) +
  theme_classic() +
  xlim(-2, 2)
```

**Reduzierter Datensatz**
```{r}
moviemeans_reducted2 <- colMeans(m_user_item_r2, na.rm = TRUE)
moviemeans_reducted2 <- data.frame(moviemeans_reducted2)
ggplot(moviemeans_reducted2, aes(moviemeans_reducted2)) +
  geom_density() +
  labs(
    title = "Streuung von durchschnittlichen Bewertung von Filmen",
    subtitle = "reduzierter Datensatz 2",
    x = "durchschnittliche Bewertung",
    y = "Verteilung"
  ) +
  theme_classic() +
  xlim(-2, 2)
```
**Beschreibung:**
Hier wird die Streuung der durchschnittlichen Bewertung einzelner Filme visualisiert. Es wird dabei der reduzierte Datensatz mit dem kompletten Datensatz verglichen.

**Schlussfolgerung:**
Man erkennt, dass die Daten beim reduzierten Datensatz grösstenteils nur im Bereich [-1, 1] streuen. Dies ist auch realistisch da es wahrscheinlicher ist, dass ein Film welcher nur 1 Mal bewertet wurde eine Bewertung von z.B. -2 hat, als dass 10 User den gleichen Film so schlecht bewerten, dass der Durchschnitt bei -2 liegt.

## Quantifiziere “Intersection over Union” der Ratings der unterschiedlich reduzierten Datensätze.
```{r}
intersection <- nrow(inner_join(user_item_r1, user_item_r2, by = c("movieId", "userId")))
union <- nrow(user_item_r1) + nrow(user_item_r2) - intersection
intersection_over_union <- round(intersection / union, 2)
```

**Beschreibung:**
Die "Intersection over Union" ist `r intersection_over_union * 100` %.
Die berechnete Zahl bezeichnet das Verhältnis von Bewertungen, welche in beiden Datensätzen vorhanden ist.

**Schlussfolgerung:**
Die Schnittmenge der Bewertungen zwischen beiden Datensätzen beträgt etwa 30% der Gesamtmenge. Das bedeutet, dass die beiden Datensätze ähnliche Daten beinhalten. Trotzdem sollten weiteren Berechnungen unterschiedliche Resultate erzielen.

# Analyse Ähnlichkeitsmatrix
## Zerlege den reduzierten MovieLense Datensatz in ein disjunktes Trainings- und Testdatenset im Verhältnis 4:1
### Sample 1
```{r}
set.seed(69)
split1 <- initial_split(m_user_item_r1, prop = 0.80)
training1 <- as.matrix(training(split1))
test1 <- as.matrix(testing(split1))
```

### Sample 2
```{r}
set.seed(100)
split2 <- initial_split(m_user_item_r2, prop = 0.80)
training2 <- as.matrix(training(split2))
test2 <- as.matrix(testing(split2))
```

## Trainiere ein IBCF Modell mit 30 Nachbarn und Cosine Similarity
### Sample 1
```{r}
IBCF1 <- Recommender(as(training1, "realRatingMatrix"), "IBCF",
  param = list(normalize = NULL, method = "cosine", k = 30, na_as_zero = TRUE)
)
```

### Sample 2
```{r}
IBCF2 <- Recommender(as(training2, "realRatingMatrix"), "IBCF",
  param = list(normalize = NULL, method = "cosine", k = 30, na_as_zero = TRUE)
)
```

## Bestimme die Verteilung der Filme, welche bei IBCF für paarweise Ähnlichkeitsvergleiche verwendet werden
### Sample 1
```{r}
# extract IBCF similarity matrix
IBCF_sim_matrix1 <- as.data.frame(as.matrix(IBCF1@model[["sim"]]))

# count number of occurrences
IBCF_freq1 <- as.data.frame(colSums(IBCF_sim_matrix1 != 0), optional = TRUE)
colnames(IBCF_freq1) <- "frequency"
ggplot(IBCF_freq1, aes(frequency)) +
  geom_histogram(bins = 30) +
  labs(
    title = "Verteilung des Vorkommens der Filme in den Top 30 Ähnlichkeitslisten",
    x = "Anzahl Vorkommen des Filmes in der Ähnlichkeitsmatrix",
    y = "Anzahl Filme"
  ) +
  theme_classic()
```

### Sample 2
```{r}
# extract IBCF similarity matrix
IBCF_sim_matrix2 <- as.data.frame(as.matrix(IBCF2@model[["sim"]]))

# count number of occurrences
IBCF_freq2 <- as.data.frame(colSums(IBCF_sim_matrix2 != 0), optional = TRUE)
colnames(IBCF_freq2) <- "frequency"
ggplot(IBCF_freq2, aes(frequency)) +
  geom_histogram(bins = 30) +
  labs(
    title = "Verteilung des Vorkommens der Filme in den Top 30 Ähnlichkeitslisten",
    x = "Anzahl Vorkommen des Filmes in der Ähnlichkeitsmatrix",
    y = "Anzahl Filme"
  ) +
  theme_classic()
```

**Beschreibung:**
Hier wird die Verteilung visualisiert, wie viele Filme wie oft in der Top 30 Ähnlichkeitsliste der anderen Filmen auftreten.

**Schlussfolgerung:**
Hier sieht die Verteilung bei beiden Datensätzen ähnlich aus. Die Verteilungen sind rechtschief mit einem Durchschnitt von 30. Somit gibt es einzelne Filme die zu vielen anderen Filmen ähnlich sind. Diese werden im nächsten Abschnitt genauer untersucht.

## Bestimme die Filme, die am häufigsten in der Cosine-Ähnlichkeitsmatrix auftauchen und analysiere deren Vorkommen und Ratings im reduzierten Datensatz.
### Häufigkeit der Filme
#### Sample 1
```{r}
# Add movieId as column
IBCF_freq1$movieId <- rownames(IBCF_freq1)

# sort by frequency, select most frequent movies
IBCF_freq_head1 <- IBCF_freq1 %>%
  arrange(desc(frequency)) %>%
  head(30) %>%
  convert(int(movieId))

# count occurrency and the mean rating of the reduced data
IBCF_freq_head1 <- left_join(IBCF_freq_head1, norm_ratings1, by = "movieId") %>%
  group_by(movieId) %>%
  summarise(
    count = n(),
    mean = mean(z_rating)
  )

ggplot(IBCF_freq_head1, aes(count)) +
  geom_histogram(binwidth = 5) +
  labs(
    title = "Anzahl Ratings der 30 meist vorgeschlagenen Filme",
    x = "Anzahl Ratings",
    y = "Anzahl Filme"
  ) +
  xlim(0, NA) +
  theme_classic()
```

#### Sample 2
```{r}
# Add movieId as column
IBCF_freq2$movieId <- rownames(IBCF_freq2)

# sort by frequency, select most frequent movies
IBCF_freq_head2 <- IBCF_freq2 %>%
  arrange(desc(frequency)) %>%
  head(30) %>%
  convert(int(movieId))

# count occurrency and the mean rating of the reduced data
IBCF_freq_head2 <- left_join(IBCF_freq_head2, norm_ratings2, by = "movieId") %>%
  group_by(movieId) %>%
  summarise(
    count = n(),
    mean = mean(z_rating)
  )

ggplot(IBCF_freq_head2, aes(count)) +
  geom_histogram(binwidth = 5) +
  labs(
    title = "Anzahl Ratings der 30 meist vorgeschlagenen Filme",
    x = "Anzahl Ratings",
    y = "Anzahl Filme"
  ) +
  xlim(0, NA) +
  theme_classic()
```

### Beschreibung:
Aus den Top 30 ähnlichen Filmen aller Filme, werden die Top 30 Filme mit den meisten Vorkommen ausgewählt und danach die Anzahl Ratings dieser Filme visualisiert.

Die meist vorgeschlagenen Filme haben in beiden reduzierten Datensätze 20 oder mehr Ratings. Bei beiden Datensätzen haben die meisten Filme zwischen 20 und 50 Ratings.

### Durchschnittliche Ratings der Filme
#### Sample 1
```{r}
ggplot(IBCF_freq_head1, aes(mean)) +
  geom_density() +
  geom_vline(xintercept = 0, alpha = 0.5, color = "magenta") +
  labs(
    title = "Verteilung der normierten Bewertungen der 30 meist vorgeschlagenen Filme",
    subtitle = paste("Mittelwert: ", round(mean(IBCF_freq_head1$mean), 2)),
    x = "Normierte Bewertung",
    y = "Verteilung"
  ) +
  theme_classic()
```

#### Sample 2
```{r}
ggplot(IBCF_freq_head2, aes(mean)) +
  geom_density() +
  geom_vline(xintercept = 0, alpha = 0.5, color = "magenta") +
  labs(
    title = "Verteilung der normierten Bewertungen der 30 meist vorgeschlagenen Filme",
    subtitle = paste("Mittelwert: ", round(mean(IBCF_freq_head2$mean), 2)),
    x = "Normierte Bewertung",
    y = "Verteilung"
  ) +
  theme_classic()
```

**Beschreibung:**
Hier wird die Verteilung der normierten Bewertungen der 30 meist vorgeschlagenen Filme visualisiert. Weiterhin haben wir eine vertikale Linie implementiert, welche die Boundary zwischen einer guten (rechts) und einer schlechten (links) Bewertung bildet.

**Schlussfolgerung:**
Die 30 am meist vorgeschlagenen Filme tendieren dazu, überdurchschnittlich gut bewertete Filme zu sein. Das können wir daraus formulieren, das der Mittelwert der Bewertungen beider Datensätze über den Wert 0 liegen.

# Implementierung Ähnlichkeitsmatrix
## Implementierung
### Berechnung der cosinus/jaccard similarity
```{r}
calculate_jaccard <- function(arr1, arr2) {
  # Check which columns are available
  vals <- (!is.na(array(arr1)) & !is.na(array(arr2)))
  # Remove movieId column from jaccard similarity
  vals[1] <- FALSE
  # If there are common not na values, calculate jac sim
  if (sum(vals) != 0) {
    both_true <- arr1[vals] & arr2[vals]
    either_true <- arr1[vals] | arr2[vals]
    jac_sim <- sum(both_true) / sum(either_true)
    return(jac_sim)
  }
  # If not, return NA
  return(NA)
}

calculate_cos <- function(arr1, arr2) {
  # Check which columns are available
  vals <- (!is.na(array(arr1)) & !is.na(array(arr2)))
  # Remove movieId column from cos similarity
  vals[1] <- FALSE
  # If there are common not na values, calculate cos sim
  if (sum(vals) != 0) {
    arr1 <- arr1[vals]
    arr2 <- arr2[vals]
    ab <- as.matrix(arr1) %*% t(arr2)
    norma <- norm(arr1, type = "2")
    normb <- norm(arr2, type = "2")
    cos_sim <- ((ab / (norma * normb)) + 1) / 2
    return(cos_sim)
  }
  # If not, return NA
  return(NA)
}
```


### Berechnung der Ähnlichkeitsmatrix
```{r}
getCorrelationMatrix <- function(data, cos = TRUE) {
  # Get array with movieId's
  movies <- as.character(data$movieId)

  # Create correlation matrix and set diag to 1
  correlations <- matrix(
    NA,
    nrow = length(movies),
    ncol = length(movies),
    dimnames = list(movies, movies)
  )
  diag(correlations) <- 1

  # Iterate through every movie and preload column
  i_counter <- 0
  for (i in movies) {
    i_counter <- i_counter + 1
    row_i <- data %>% filter(movieId == i)
    # For every movie, iterate through every movie
    j_counter <- 0
    for (j in movies) {
      j_counter <- j_counter + 1
      # If cos similarity was already calculated, skip, else continue
      if (i_counter <= j_counter) {
        # calculate similarity
        row_j <- data %>% filter(movieId == j)
        if (cos) {
          sim <- calculate_cos(row_i, row_j)
        } else {
          sim <- calculate_jaccard(row_i, row_j)
        }
        # set sim in sim matrix
        correlations[i, j] <- sim
        correlations[j, i] <- sim
      }
    }
    # Track progress
    # print(paste(i_counter, " Datasets done"))
  }
  # Return correlation matrix
  return(correlations)
}

numToBool <- function(x) (x >= 0)
```

### Effiziente Berechnung der Ähnlichkeitsmatrix (cos sim), wenn man NA's mit 0 ersetzt
```{r}
getCorrelationMatrixCosNoNA <- function(data, cos = TRUE) {
  data[is.na(data)] <- 0
  data <- t(data)
  AAT <- data %*% t(data)
  norm_ <- rep(NA, nrow(data))
  for (i in 1:nrow(data)) {
    norm_[i] <- sqrt(sum(data[i, ]^2))
  }
  norms <- norm_ %*% t(norm_)
  result <- AAT / norms
  return((result + 1) / 2)
}
```

## Berechnung der Ähnlichkeitsmatrizen
### Sample 1
```{r}
# Erstellung der User-Rating Matrix
set.seed(100)
sample_values1 <- sample(1:6819, 300, replace = FALSE)

norm_ratings1 <- ratings1 %>%
  group_by(userId) %>%
  summarise(mean_rating = mean(rating), sd_rating = sd(rating)) %>%
  full_join(., ratings1, by = "userId")

norm_ratings1$z_rating <- (norm_ratings1$rating - norm_ratings1$mean_rating) /
  norm_ratings1$sd_rating

item_user_random_100_1 <- norm_ratings1 %>%
  select(movieId, userId, z_rating) %>%
  pivot_wider(names_from = userId, values_from = z_rating) %>%
  filter(movieId %in% sample_values1) %>%
  head(100)

item_user_random_100_bool1 <- item_user_random_100_1 %>% mutate(across(!matches("movieId"), numToBool))
```

```{r}
corrNumb1 <- getCorrelationMatrix(item_user_random_100_1, cos = TRUE)
corrBool1 <- getCorrelationMatrix(item_user_random_100_bool1, cos = FALSE)
```

### Sample 2
```{r}
# Erstellung der User-Rating Matrix
set.seed(100)
sample_values2 <- sample(1:6819, 300, replace = FALSE)

norm_ratings2 <- ratings2 %>%
  group_by(userId) %>%
  summarise(mean_rating = mean(rating), sd_rating = sd(rating)) %>%
  full_join(., ratings2, by = "userId")

norm_ratings2$z_rating <- (norm_ratings2$rating - norm_ratings2$mean_rating) /
  norm_ratings2$sd_rating

item_user_random_100_2 <- norm_ratings2 %>%
  select(movieId, userId, z_rating) %>%
  pivot_wider(names_from = userId, values_from = z_rating) %>%
  filter(movieId %in% sample_values1) %>%
  head(100)

item_user_random_100_bool2 <- item_user_random_100_2 %>% mutate(across(!matches("movieId"), numToBool))
```

```{r}
corrNumb2 <- getCorrelationMatrix(item_user_random_100_2, cos = TRUE)
corrBool2 <- getCorrelationMatrix(item_user_random_100_bool2, cos = FALSE)
```

## Vergleich mit recommenderlabs
### Sample 1

**Recommenderlab Ähnlichkeitsmatrix mit Cosine Similarity**
```{r}
item_user_random_100_recommenderlab1 <- item_user_random_100_1 %>%
  column_to_rownames(., var = "movieId") %>%
  as.matrix(.) %>%
  t(.)

corrNumbRL1 <- as.matrix(similarity(as(item_user_random_100_recommenderlab1, "realRatingMatrix"), method = "cosine", which = "items"))

corrNumbRL1[1:6, 1:6]
```

**Selbstgemachte Ähnlichkeitsmatrix mit Cosine Similarity**
```{r}
corrNumb1[1:6, 1:6]
```

**Recommenderlab Ähnlichkeitsmatrix mit Jaccard Similarity**
```{r}
item_user_random_100_bool_recommenderlab1 <- item_user_random_100_bool1 %>%
  column_to_rownames(., var = "movieId") %>%
  t(.)

corrBoolRL1 <- as.matrix(similarity(as(item_user_random_100_bool_recommenderlab1, "realRatingMatrix"), method = "jaccard", which = "items"))

corrBoolRL1[1:6, 1:6]
```

**Selbstgemachte Ähnlichkeitsmatrix mit Jaccard Similarity**
```{r}
corrBool1[1:6, 1:6]
```

### Sample 2

**Recommenderlab Ähnlichkeitsmatrix mit Cosine Similarity**
```{r}
item_user_random_100_recommenderlab2 <- item_user_random_100_2 %>%
  column_to_rownames(., var = "movieId") %>%
  as.matrix(.) %>%
  t(.)

corrNumbRL2 <- as.matrix(similarity(as(item_user_random_100_recommenderlab2, "realRatingMatrix"), method = "cosine", which = "items"))

corrNumbRL2[1:6, 1:6]
```

**Selbstgemachte Ähnlichkeitsmatrix mit Cosine Similarity**
```{r}
corrNumb2[1:6, 1:6]
```

**Recommenderlab Ähnlichkeitsmatrix mit Jaccard Similarity**
```{r}
item_user_random_100_bool_recommenderlab2 <- item_user_random_100_bool2 %>%
  column_to_rownames(., var = "movieId") %>%
  t(.)

corrBoolRL2 <- as.matrix(similarity(as(item_user_random_100_bool_recommenderlab2, "realRatingMatrix"), method = "jaccard", which = "items"))

corrBoolRL2[1:6, 1:6]
```

**Selbstgemachte Ähnlichkeitsmatrix mit Jaccard Similarity**
```{r}
corrBool2[1:6, 1:6]
```


### Observationen
Recommenderlab erstellt ähnliche Ähnlichkeitsmatrizen wie wir. Der grösste Unterschied ist, dass wir die Diagonale mit der Korrelation 1 befüllen. Recommenderlab hingegen schreibt NA auf die Diagonale. Weiterhin sind kleine Rundungsfehler sichtbar, jedoch sind diese eher klein. Bei der Jaccard Korrelationsmatrix sehen wir auch, dass wenn kein Korrelationswert berechnet werden kann, dass Recommenderlab eine Korrelation von 1 zurückgibt. Wir geben da NA zurück.

## Vergleich mit recommenderlab (cosine sim, NA = 0)
### Sample 1

**Recommenderlab Ähnlichkeitsmatrix mit Cosine Similarity**
```{r}
item_user_random_100_recommenderlab_NoNA1 <- item_user_random_100_recommenderlab1
item_user_random_100_recommenderlab_NoNA1[is.na(item_user_random_100_recommenderlab_NoNA1)] <- 0

coorNumbNoNA1 <- getCorrelationMatrixCosNoNA(item_user_random_100_recommenderlab_NoNA1)
corrNumbNoNARL1 <- as.matrix(similarity(as(item_user_random_100_recommenderlab_NoNA1, "realRatingMatrix"), method = "cosine", which = "items"))

corrNumbNoNARL1[1:6, 1:6]
```

**Selbstgemachte Ähnlichkeitsmatrix mit Cosine Similarity**
```{r}
coorNumbNoNA1[1:6, 1:6]
```

### Sample 2
**Recommenderlab Ähnlichkeitsmatrix mit Cosine Similarity**
```{r}
item_user_random_100_recommenderlab_NoNA2 <- item_user_random_100_recommenderlab2
item_user_random_100_recommenderlab_NoNA2[is.na(item_user_random_100_recommenderlab_NoNA2)] <- 0

coorNumbNoNA2 <- getCorrelationMatrixCosNoNA(item_user_random_100_recommenderlab_NoNA2)
corrNumbNoNARL2 <- as.matrix(similarity(as(item_user_random_100_recommenderlab_NoNA2, "realRatingMatrix"), method = "cosine", which = "items"))

corrNumbNoNARL2[1:6, 1:6]
```

**Selbstgemachte Ähnlichkeitsmatrix mit Cosine Similarity**
```{r}
coorNumbNoNA2[1:6, 1:6]
```

### Observationen
Wenn wir bei der Item User Matrix NAs mit 0 ersetzen, können wir unseren schnelleren Algorithmus verwenden. Dieser Algorythmus ist wieder der fast identisch zu Recommenderlabs (wie beim vorherigen). Jedoch erhält man andere Werte wenn man NAs mit 0 ersetzt, da man annimmt, dass nicht bewertete Filme neutral bewertet werden.

## Vergleich mit coop (cosine) und vegan (jaccard)
### Sample 1

**Coop Ähnlichkeitsmatrix mit Cosine Similarity**
```{r}
corrNumbC1 <- coop::cosine(item_user_random_100_recommenderlab1)

corrNumbC1[1:6, 1:6]
```

**Selbstgemachte Ähnlichkeitsmatrix mit Cosine Similarity**
```{r}
corrNumb1[1:6, 1:6]
```

**vegan Ähnlichkeitsmatrix mit Jaccard Similarity**
```{r}
corrBoolVG1 <- 1 - vegdist(item_user_random_100_bool_recommenderlab1 %>% t(.), method = "jaccard", na.rm = TRUE) %>%
  as.matrix(.)

corrBoolVG1[1:6, 1:6]
```

**Selbstgemachte Ähnlichkeitsmatrix mit Jaccard Similarity**
```{r}
corrBool1[1:6, 1:6]
```


### Sample 2

**Coop Ähnlichkeitsmatrix mit Cosine Similarity**
```{r}
corrNumbC2 <- coop::cosine(item_user_random_100_recommenderlab2)

corrNumbC2[1:6, 1:6]
```

**Selbstgemachte Ähnlichkeitsmatrix mit Cosine Similarity**
```{r}
corrNumb2[1:6, 1:6]
```

**vegan Ähnlichkeitsmatrix mit Jaccard Similarity**
```{r}
corrBoolVG2 <-
  1 - vegdist(
    item_user_random_100_bool_recommenderlab2 %>% t(.),
    method = "jaccard",
    na.rm = TRUE
  ) %>%
  as.matrix(.)

corrBoolVG2[1:6, 1:6]
```

**Selbstgemachte Ähnlichkeitsmatrix mit Jaccard Similarity**
```{r}
corrBool2[1:6, 1:6]
```

### Observationen
Leider kann die cosine Funktion von coop die Ähnlichkeiten nicht berechnen, wenn in der Matrix NA Werte vorhanden sind. Dafür rechnet die vegdist Funktion von vegan die Dissimilarity Matrix genau aus. Diese muss man noch mit (X = 1 - X) umkehren, damit sie wie unsere Similarity Matrix aussieht.

## Vergleich der Korrelationsmatrizen
Die Korrelationsmatrix mit ordinalen Ratings scheint viel detailliertere Korrelationswerte zurückzugeben, da wir genaue Ratings der User haben. Da mit der Umwandlung zu binären Werten diese Informationen verloren gehen, sieht die Korrelationsmatrix mit binären Werten dementsprechend weniger "hochauflösend" (kann durch einfache Bruchzahlen interpretiert werden) aus.


# Analyse Top-N Listen - IBCF vs UBCF
## Berechne Top-15 Empfehlungen für Testkunden mit IBCF und UBCF
### Sample 1
```{r}
# predict IBCF
pIBCF1 <- predict(IBCF1, as(test1, "realRatingMatrix"), type = "topNList", n = 15)
```

```{r}
# calc frequency of predicted movies
freq_pred_IBCF1 <- table(unlist(as(pIBCF1, "list"))) %>%
  as.data.frame() %>%
  rename(movieId = Var1) %>%
  arrange(desc(Freq))
```

```{r}
# train UBCF
UBCF1 <- Recommender(as(training1, "realRatingMatrix"), "UBCF",
  param = list(normalize = NULL, method = "cosine", nn = 30)
)

# predict UBCF
pUBCF1 <- predict(UBCF1, as(test1, "realRatingMatrix"), type = "topNList", n = 15)
```

```{r}
# calc frequency of predicted movies
freq_pred_UBCF1 <- table(unlist(as(pUBCF1, "list"))) %>%
  as.data.frame() %>%
  rename(movieId = Var1) %>%
  arrange(desc(Freq))
```

### Sample 2
```{r}
# predict IBCF
pIBCF2 <- predict(IBCF2, as(test2, "realRatingMatrix"), type = "topNList", n = 15)
```

```{r}
# calc frequency of predicted movies
freq_pred_IBCF2 <- table(unlist(as(pIBCF2, "list"))) %>%
  as.data.frame() %>%
  rename(movieId = Var1) %>%
  arrange(desc(Freq))
```

```{r}
# train UBCF
UBCF2 <- Recommender(as(training2, "realRatingMatrix"), "UBCF",
  param = list(method = "cosine", nn = 30)
)

# predict UBCF
pUBCF2 <- predict(UBCF2, as(test2, "realRatingMatrix"), type = "topNList", n = 15)
```

```{r}
# calc frequency of predicted movies
freq_pred_UBCF2 <- table(unlist(as(pUBCF2, "list"))) %>%
  as.data.frame() %>%
  rename(movieId = Var1) %>%
  arrange(desc(Freq))
```

## Vergleiche die Top-15 Empfehlungen und deren Verteilung und diskutiere Gemeinsamkeiten und Unterschiede zwischen IBCF und UBCF für alle Testkunden.
**Hypothese:** Recommender Systeme machen für alle Nutzer die gleichen Empfehlungen.

### Sample 1
```{r}
freq_pred_UBCF1$type <- "UBCF"
freq_pred_IBCF1$type <- "IBCF"

moviesUIBCF1 <- rbind(freq_pred_UBCF1, freq_pred_IBCF1)
```

```{r}
ggplot(moviesUIBCF1, aes(Freq, fill = type)) +
  geom_histogram(alpha = 0.6, position = "dodge", binwidth = 0.5) +
  scale_fill_manual(values = c("#69b3a2", "#404080")) +
  labs(
    title = "Häufigkeit der Empfehlung von Filmen",
    x = "Anzahl Empfehlungen pro Film",
    y = "Anzahl Filme",
    fill = "Modell"
  ) +
  theme_classic()
```

### Sample 2
```{r}
freq_pred_UBCF2$type <- "UBCF"
freq_pred_IBCF2$type <- "IBCF"

moviesUIBCF2 <- rbind(freq_pred_UBCF2, freq_pred_IBCF2)
```

```{r}
ggplot(moviesUIBCF2, aes(Freq, fill = type)) +
  geom_histogram(alpha = 0.6, position = "dodge", binwidth = 0.5) +
  scale_fill_manual(values = c("#69b3a2", "#404080")) +
  labs(
    title = "Häufigkeit der Empfehlung von Filmen",
    x = "Anzahl Empfehlungen pro Film",
    y = "Anzahl Filme",
    fill = "Modell"
  ) +
  theme_classic()
```

**Beschreibung:**
Hier wird visualisiert, wie oft Filme bei IBCF und UBCF empfohlen werden. Auf der x-Achse befindet sich die Anzahl Empfehlungen pro Film und auf der y-Achse wieviele oft die Anzahl Empfehlungen pro Film gleich sind.
Der ICBF Recommender empfiehlt, dabei merh unterschiedliche Filme als der UBCF recommender. Der UCBF Recommender empfiehlt bis zu 24 mal den gleichen Film. 

**Schlussfolgerung:**
Die Hypothese wurde wiederlegt, da diverse Filme nur einmal vorgeschlagen werden. Zudem werden keine Filme mehr als 24 mal vorgeschlagen.

# Analyse Top-N Listen - Ratings
Funktion um die Überschneidungen zu berechnen.
```{r}
intersection_Top15 <- function(prediction1, prediction2, Model1, Model2, Datensatz, return_plot = FALSE) {
  predTopN1 <- as.data.frame(as(prediction1, "matrix"))
  predTopN1$user <- rownames(predTopN1)
  predTopN1 <- pivot_longer(predTopN1, cols = -c(user), values_drop_na = TRUE)

  predTopN2 <- as.data.frame(as(prediction2, "matrix"))
  predTopN2$user <- rownames(predTopN2)
  predTopN2 <- pivot_longer(predTopN2, cols = -c(user), values_drop_na = TRUE)


  intersect <- left_join(predTopN1, predTopN2, by = c("user", "name"))

  # count intersect
  plot <- intersect %>%
    select(user, value.y) %>%
    group_by(user) %>%
    summarise(total_intersect = sum(!is.na(value.y))) %>%
    ggplot(aes(total_intersect)) +
    geom_histogram(binwidth = 0.5) +
    labs(
      title = "Überschneidungen von Top-15 Listen",
      x = "Anzahl Überschneidungen der Bewertungen",
      y = "Anzahl User",
      subtitle = paste(Model1, "und", Model2),
      caption = paste("Datensatz", Datensatz)
    ) +
    scale_x_continuous(breaks = function(x) unique(floor(pretty(seq(0, (max(x) + 1) * 1.1))))) + # integer on x_axis
    theme_classic() +
    theme(legend.position = "none")
  if (return_plot) {
    return(plot)
  } else {
    print(plot)
  }
}
```

## IBCF vs UBCF, beide mit ordinalem Rating und Cosine Similarity für alle Testkunden
**Hypothese:** Basierend auf der vorherigen Aufgabe, gehen wir davon aus, dass die Anzahl übereinstimmenden Filme relativ klein ist. Da die Verteilungen bei der Aufgabe 5 unterschiedlich sind.

```{r}
intersection_Top15(pUBCF1, pIBCF1, "UBCF Cosine", "IBCF Cosine", "1")
```

```{r}
intersection_Top15(pUBCF2, pIBCF2, "UBCF Cosine", "IBCF Cosine", "2")
```
\
**Beschreibung:** Die Überschneidung der Daten ist niedrig. Die meisten Kunden haben keine Überschneidung.

**Schlussfolgerung:** Da der Algorithmus unterschiedlich ist und es diverse mögliche Filme zur Auswahl hat, sind die Top-N Listen unterschiedlich.


## IBCF vs UBCF, beide mit binärem Rating und Jaccard Similarity für alle Testkunden
**Hypothese:** Die Überschneidung ist grösser wie bei der Cosine Similarity. Trotzdem werden den meisten User unterschiedliche Filme vorgeschlagen.
```{r}
# Sample 1
# Create binary training and test data
training_binary1 <- training1 > 0
training_binary1[is.na(training_binary1)] <- 0

test_binary1 <- test1 > 0
test_binary1[is.na(test_binary1)] <- 0
```

```{r}
# Sample 2
# Create binary training and test data
training_binary2 <- training2 > 0
training_binary2[is.na(training_binary2)] <- 0

test_binary2 <- test2 > 0
test_binary2[is.na(test_binary2)] <- 0
```

```{r}
# sample 1
# Train and test binary UBCF-recommender
UBCF_binary1 <- Recommender(as(training_binary1, "realRatingMatrix"), "UBCF", param = list(normalize = NULL, method = "jaccard"))
pUBCF_binary1 <- predict(UBCF_binary1, as(test1, "realRatingMatrix"), type = "topNList", n = 15)

# Train and test binary IBCF-recommender
IBCF_binary1 <- Recommender(as(training_binary1, "realRatingMatrix"), "IBCF", param = list(normalize = NULL, method = "jaccard"))
pIBCF_binary1 <- predict(IBCF_binary1, as(test1, "realRatingMatrix"), type = "topNList", n = 15)
# calculate and plot intersection
intersection_Top15(pUBCF_binary1, pIBCF_binary1, "UBCF binary", "IBCF binary", "1")
```

```{r}
# sample 2
# Train and test binary UBCF-recommender
UBCF_binary2 <- Recommender(as(training_binary2, "realRatingMatrix"), "UBCF", param = list(normalize = NULL, method = "jaccard"))
pUBCF_binary2 <- predict(UBCF_binary2, as(test2, "realRatingMatrix"), type = "topNList", n = 15)

# Train and test binary IBCF-recommender
IBCF_binary2 <- Recommender(as(training_binary2, "realRatingMatrix"), "UBCF", param = list(normalize = NULL, method = "jaccard"))
pIBCF_binary2 <- predict(IBCF_binary1, as(test2, "realRatingMatrix"), type = "topNList", n = 15)

# calculate and plot intersection
intersection_Top15(pUBCF_binary2, pIBCF_binary2, "UBCF binary", "IBCF binary", "2")
```
\
**Beschreibung:** Bei den meisten Usern gibt es keine Überschneidung, bei den unterschiedlichen Recommender. Bei diesen Algorithm gibt es User mit 6 Überschneidungen.

**Schlussfolgerung** Die Hypothese wurde bestätigt. Die Überschneidungen sind grösser wie bei den Recommender mit Kosinus Ähnlichkeiten. Trotzdem hat es im Datensatz 2 fast 60 User die keine Überschneidungen haben.


## UBCF mit ordinalem (Cosine Similarity) vs UBCF mit binärem Rating (Jaccard Similarity) für alle Testkunden.
**Hypothese:** Die beiden Recommender sind unterschiedlicher wie die bis anhin untersuchten. Somit sollten sie deutlich weniger Überschneidungen aufweisen.

```{r}
intersection_Top15(pUBCF1, pUBCF_binary1, "UBCF binary", "UBCF cosine", "1")
```

```{r}
intersection_Top15(pUBCF2, pUBCF_binary2, "UBCF binary", "UBCF cosine", "2")
```
\
**Beschreibungen:** Es hat fast keine Überschneidungen. Im zweiten Datensatz hat ein User 5 Filme in beiden Top-N Listen.

**Schlussfolgerung:** Die Hypothese kann angenommen werden. Es hat aber nur leicht weniger Überschneidungen, wie beim Vergleich mit ordinalen Ratings und Kosinus Ähnlichkeiten.

## IBCF (Na_as_zeros = FALSE) vs IBCF (Na_as_zeros = TRUE)

Zusätzlich zu den Aufgaben wird noch überprüft ob es ein Unterschied macht wenn die NA gelöscht werden. (Anstatt mit 0 überschrieben)

**Hypothese:** Die Überschneidungen sollten gross sein.  

```{r}
IBCF1_na_false <- Recommender(as(training2, "realRatingMatrix"), "IBCF",
  param = list(normalize = NULL, method = "cosine", k = 30, na_as_zero = FALSE)
)
pIBCF1_na_false <- predict(IBCF1_na_false, as(test1, "realRatingMatrix"), type = "topNList", n = 15)

intersection_Top15(pIBCF1_na_false, pIBCF1, "UBCF cosine mit na", "UBCF cosine ohne na", "1")
```

```{r}
IBCF2_na_false <- Recommender(as(training2, "realRatingMatrix"), "IBCF",
  param = list(normalize = NULL, method = "cosine", k = 30, na_as_zero = FALSE)
)
pIBCF2_na_false <- predict(IBCF2_na_false, as(test2, "realRatingMatrix"), type = "topNList", n = 15)
intersection_Top15(pIBCF2_na_false, pIBCF2, "UBCF cosine mit na", "UBCF cosine ohne na", "2")
```

**Beschreibung:** Im ersten Datensatz sind die Überschneidungen klein. Die meisten User haben 0 Überschneidungen. Im zweiten Datensatz hat es deutlich mehr Überschneidungen.

**Schlussfolgerung:** Die Hypothese wird abgelehnt. Es hat einen Einfluss, ob die NA-Werte auf 0 gesetzt werden oder gelöscht werden. Wenn die NA auf 0 gesetzt werden wird die Ähnlichkeit tendenziell kleiner. Da bei der Kosinus Ähnlichkeit der Nenner grösser wird und der Zähler nicht. Es macht Sinn das die Ähnlichkeit kleiner ist, wenn es wenig übereinstimmende Ratings hat.

## Allgemeine Schlussfolgerung

Bei dieser Aufgabe wurde erkannt, dass das Resultat je nach Einstellungen des Recommender, sehr unterschiedlich sein kann. Um herauszufinden welcher Recommender gut ist, müssen diese Evaluiert werden. Dies wird in den Aufgabe 9 und 10 genauer untersucht.


# Analyse Top-N Listen - IBCF vs SVD
In diesem Abschnitt wird ein IBCF Recommender mit einem SVD Recommender verglichen. Der SVD Recommender wird mit 10,20,30,40 und 50 Singulärwerten ausgeführt.

**Hypothese** Je mehr Singulärwerte verwendet werden, desto grösser werden die Überschneidungen
```{r}
SVD_pred <- function(train_data, test_data, singular_value) {
  SVD1 <- Recommender(as(train_data, "realRatingMatrix"), "SVD",
    param = list(k = singular_value)
  )
  PSVD1 <- predict(SVD1, as(test_data, "realRatingMatrix"), type = "topNList", n = 15)
  return(PSVD1)
}
```

```{r,fig.width=9,fig.height=12}
# Sample 1
p <- list()
i <- 1
for (singular_value in c(10, 20, 30, 40, 50)) {
  p[[i]] <- intersection_Top15(pIBCF1, SVD_pred(training1, test1, singular_value), paste("SVD mit ", singular_value, " Singulärwerten"), "IBCF", "1", TRUE)
  i <- i + 1
}
do.call(grid.arrange, c(p, ncol = 2))
```

```{r,fig.width=9,fig.height=12}
# Sample 2
p <- list()
i <- 1
for (singular_value in c(10, 20, 30, 40, 50)) {
  p[[i]] <- intersection_Top15(pIBCF2, SVD_pred(training2, test2, singular_value), paste("SVD mit ", singular_value, " Singulärwerten"), "IBCF", "2", TRUE)
  i <- i + 1
}
do.call(grid.arrange, c(p, ncol = 2))
```
**Beschreibung:** Die Überschneidungen sind im allgemeinen nicht gross. Mit steigendem Singulärwert nehmen die Überschneidungen zu. Im Datensatz 1 nehmen sie stärker zu wie im Datensatz 2

**Schlussfolgerung:** Die Hypothese kann somit angenommen werden. Je weniger die Daten komprimiert (mehr Singulärwerte) werden, desto mehr Überschneidungen sind mit dem IBCF Recommender vorhanden.

# Implementierung Top-N Metriken
Die Metriken werden nach folgenden Fomeln implementiert
https://spaces.technik.fhnw.ch/spaces/recommender-systems/beitraege/recommender-system-evaluierung-coverage-und-novelty-1

## Funktion Coverage
```{r}
coverage <- function(model) {
  predTopN <- as.data.frame(as(model, "matrix"))
  predTopN$user <- rownames(predTopN)
  predTopN <- pivot_longer(predTopN, cols = -c(user), values_drop_na = TRUE)

  return(length(unique(predTopN$name)) / 700)
}
```

```{r}

calc_IBCF <- function(data, top_n) {
  IBCF_full1 <- Recommender(as(as.matrix(data), "realRatingMatrix"), "IBCF",
    param = list(normalize = NULL, method = "cosine", k = 30, na_as_zero = TRUE)
  )
  pIBCF_full1 <- predict(IBCF_full1, as(as.matrix(data), "realRatingMatrix"), type = "topNList", n = top_n)
  return(pIBCF_full1)
}
```

**Kurzbeschrieb Coverage**

Die Metrik Coverage zeigt wie Gross der Anteil von allen Filmen ist, die vorgeschlagen werden. Anhand von dieser Metrik kann man gut erkennen, ob die meisten Filmen für einen Vorschlag in Betracht gezogen werden.
Mit dieser Metrik kann nicht getestet werden, wie oft die einzelnen Filme vorgekommen sind. Es könnte immer noch sein, dass ein Film bei allen Top-N Listen vorkommt.

## Funktion Novelty
```{r}
# calc popularity sample 1
log_popularity1 <- ratings1 %>%
  group_by(movieId) %>%
  summarise(log_pop = log(n() / 400))
```

```{r}
# calc popularity sample 2
log_popularity2 <- ratings2 %>%
  group_by(movieId) %>%
  summarise(log_pop = log(n() / 400))
```

```{r}
novelty <- function(model, log_popularity, topn, num_user = 400) {
  predTopN <- as.data.frame(as(model, "matrix"))
  predTopN$user <- rownames(predTopN)
  predTopN <- pivot_longer(predTopN, cols = -c(user), values_drop_na = TRUE) %>%
    rename(movieId = name)
  predTopN <- transform(predTopN, movieId = as.numeric(movieId))
  pred_with_log <- left_join(predTopN, log_popularity, by = "movieId")
  return(-sum(pred_with_log$log_pop) / num_user / topn)
}
```

**Kurzbeschrieb Novelty**

Mit dieser Metrik kann aufgezeigt werden, wieviel neues eine Recommender hervorbringt. Je grösser die Novelty ist, desto mehr bringt ein Recommender neues hervor. Durch die Popularity wird ein Bezug auf die Testdaten genommen.

## Test von den System-Metriken
```{r}
novelty1 <- c()
novelty2 <- c()
coverage1 <- c()
coverage2 <- c()
topn <- c(5, 10, 15, 20, 25, 30)
for (TopN in topn) {
  novelty1 <- append(novelty1, novelty(calc_IBCF(m_user_item_r1, TopN), log_popularity1, TopN))
  coverage1 <- append(coverage1, coverage(calc_IBCF(m_user_item_r1, TopN)))
  novelty2 <- append(novelty2, novelty(calc_IBCF(m_user_item_r2, TopN), log_popularity2, TopN))

  coverage2 <- append(coverage2, coverage(calc_IBCF(m_user_item_r2, TopN)))
}
```


```{r}
cov_novel1 <- as.data.frame(list(coverage1, novelty1, topn), col.names = c("cov", "novel", "topn"))
cov_novel2 <- as.data.frame(list(coverage2, novelty2, topn), col.names = c("cov", "novel", "topn"))
```

```{r}
ggplot(cov_novel1, aes(topn, cov)) +
  geom_point() +
  ylim(0, 1) +
  labs(
    title = "Verlauf von Coverage jenach länge der TOP-N Liste",
    x = "Länge Top-N Liste",
    y = "Coverage",
    subtitle = "Datensatz 1"
  ) +
  theme_classic()
```

```{r}
ggplot(cov_novel2, aes(topn, cov)) +
  geom_point() +
  ylim(0, 1) +
  labs(
    title = "Verlauf von Coverage jenach länge der TOP-N Liste",
    x = "Länge Top-N Liste",
    y = "Coverage",
    subtitle = "Datensatz 2"
  ) +
  theme_classic()
```

**Beschreibung Coverage**

Die Coverage steigt bis zu der Top- 15 Liste stark an. Danach bleibt die Coverage bei nahezu 1 bestehen.

**Interpretation**

Der Recommender deckt ab einer Top- 15 Liste nahezu alle Filme ab. Somit gibt es keine grössere Anzahl von Filmen, welche ausgelassen werden.

```{r}
ggplot(cov_novel1, aes(topn, novel)) +
  geom_point() +
  ylim(0, 2.5) +
  labs(
    title = "Verlauf von Novelty jenach länge der TOP-N Liste",
    x = "Länge Top-N Liste",
    y = "Novelty",
    subtitle = "Datensatz 1"
  ) +
  theme_classic()
```


```{r}
ggplot(cov_novel2, aes(topn, novel)) +
  geom_point() +
  ylim(0, 2.5) +
  labs(
    title = "Verlauf von Novelty jenach länge der TOP-N Liste",
    x = "Länge Top-N Liste",
    y = "Novelty",
    subtitle = "Datensatz 2"
  ) +
  theme_classic()
```

**Beschreibung Novelty**

Die Novelty steigt im Verlauf kaum an. Bei beiden Datensätzen wird eine Novelty von etwa 2.1 erreicht.

**Interpretation**

Die Novelty ist eine Metrik, welches gut für Vergleiche geeignet ist. Es schwierig zu sagen, was eine Novelty von 2.5 bedeutet. Spannend zu sehen, ist das die Novelty unabhängig von der Länge der Top-N Liste ist.

# Wahl des optimalen Recommenders

## 10-fache Kreuzvalidierung

Im folgenden wird eine 10-fache Kreuzvalidierung. Damit eine Aussage gemacht werden kann, wurde eine minimale Anzahl von 3 Ratings bestimmt. Diese sollten jeweils in den Trainingsdaten vorhanden sein.

```{r}
# sample 1
scheme1 <- evaluationScheme(as(as.matrix(m_user_item_r1), "realRatingMatrix"), method = "cross-validation", k = 10, given = 3, goodRating = 0)
scheme_binary1 <- evaluationScheme(binarize(as(as.matrix(m_user_item_r2), "realRatingMatrix"), 0), method = "cross-validation", k = 10, given = 3, goodRating = 0)
```
```{r}
# sample 2
scheme2 <- evaluationScheme(as(as.matrix(m_user_item_r2), "realRatingMatrix"), method = "cross-validation", k = 10, given = 3, goodRating = 0)
```
## Begründung der Modelle und Metriken

**Modelle:**
In dieser Aufgabe untersuchen wir die bereits kennengelernten Modelle. Für einen besseren Test, müssten die Hyperparameter der einzelnen Modelle zuerst trainiert werden. Dies wird aus Zeitgründen nicht gemacht. Trotzdem werden ähnliche Parameter gleich verwendet. Somit werden zum Beispiel beim UBCF und IBCF Recommender jeweils 30 Neighbours verwendet.

**Metriken:**
Als Testmetriken wird Precision, Novelty und Coverage verwendet. Precision wird verwendet, damit möglichst viele gute Filme in den Top-N Listen vorhanden sind. Novelty damit neue Erkentnisse aus den Daten gezogen werden können und Coverage um zu sehen, wie gross der Anteil von Filmen ist, die abgedeckt werden.

## Analyses des besten Modelles

### Precision

```{r, results='hide'}
# Sample 1
algorithms <- list(
  "user-based CF" = list(name = "UBCF", param = list(
    method = "Cosine",
    nn = 20
  )),
  "item-based CF" = list(name = "IBCF", param = list(
    k = 20,
    method = "cosine"
  )),
  "Binary user-based CF" = list(name = "IBCF", param = list(
    k = 20,
    method = "jaccard"
  )),
  "Binary item-based CF" = list(name = "UBCF", param = list(
    nn = 20,
    method = "jaccard"
  )),
  "SVD" = list(name = "SVD", param = list(k = 30))
)

# run algorithms, predict next n movies
topn <- c(10, 15, 20, 25, 30)
results <- evaluate(scheme1, algorithms, type = "topNList", n = topn)
```

```{r}
# Sample 1
# Precision extrahieren
alg_names <- names(algorithms)
precision <- as.data.frame({})
for (model in 1:length(alg_names)) {
  model_select <- getResults(results[[model]])
  precision_split <- as.data.frame({})
  for (split in 1:10) {
    precision_split <- rbind(precision_split, select(as.data.frame(model_select[[split]]), c("precision", "n")))
  }
  precision_split$model <- alg_names[[model]]
  precision <- rbind(precision, precision_split)
}


# Standardabweichung und durchschnitt der 10 kfolds berechnen.
precision <- precision %>%
  group_by(model, n) %>%
  summarise(
    mean_prec = mean(precision),
    sd_prec = sd(precision)
  )
```

**Top Movie Recommender**

Der Top Movie Recommender schlägt bei jedem User die Filme vor mit dem höchsten Durchschnittsrating über dem ganzen Datensatz.

```{r}
# sample 1
# Top Movie Recommender
top_n_movies1 <- user_item_r1 %>%
  group_by(movieId) %>%
  summarise(mean_rating = mean(z_rating)) %>%
  arrange(desc(mean_rating)) %>%
  head(30) %>%
  rename(name = movieId)
class(top_n_movies1$name) <- "character"

top_n <- top_n_movies1$name

prec_best_mean <- as.data.frame({})

for (n in topn) {
  prec_fold <- list()
  for (n_split in 1:10) {
    pred_movies <- getData(scheme1, "unknown", run = n_split)
    pred_movies <- as.data.frame(as(pred_movies, "matrix"))
    pred_movies$username <- rownames(pred_movies)
    pred_movies_long <- pivot_longer(pred_movies, cols = -c(username), values_drop_na = TRUE)
    prec <- nrow(pred_movies_long %>%
      filter(name %in% top_n[1:n])) / n / 40

    prec_fold <- append(prec_fold, prec)
  }
  prec_best_mean <- rbind(prec_best_mean, as.data.frame(list(mean(unlist(prec_fold)), sd(unlist(prec_fold)), n, "Top Movie"), col.names = c("mean_prec", "sd_prec", "n", "model")))
}

precision <- rbind(precision, prec_best_mean)

precision$type <- "precision"
```

**Precision Visualisieren**

```{r,fig.width=14,fig.height=12}
# plot
p <- list()
i <- 1
for (n_value in topn) {
  pre_plot <- filter(precision, n == n_value)
  plot <- (ggplot(pre_plot, aes(x = model, y = mean_prec)) +
    geom_point() +
    geom_errorbar(aes(ymin = mean_prec - sd_prec, ymax = mean_prec + sd_prec),
      width = .2,
      position = position_dodge(.9)
    ) +
    ylim(0, 0.2) +
    labs(
      title = paste("Precision von Top -", n_value, " Listen"),
      x = "Modell",
      y = "Precision",
      caption = "Datensatz 1"
    ))
  p[[i]] <- plot

  i <- i + 1
}
do.call(grid.arrange, c(p, ncol = 2))
```

```{r}
ggplot(precision, aes(x = n, y = mean_prec,color = model)) +
  geom_point() +
    ylim(0, 0.2) +
    labs(
      title = paste("Precision von Top - n Listen"),
      x = "Länge Top-N Liste",
      y = "Precision",
      color = "Modelle",
      caption = "Datensatz 1"
    )
```


```{r, results='hide'}
# Sample 2
# run algorithms, predict next n movies
results <- evaluate(scheme2, algorithms, type = "topNList", n = topn)
```

```{r}
# Sample 2
# Precision extrahieren
precision2 <- as.data.frame({})
for (model in 1:length(alg_names)) {
  model_select <- getResults(results[[model]])
  precision_split <- as.data.frame({})
  for (split in 1:10) {
    precision_split <- rbind(precision_split, select(as.data.frame(model_select[[split]]), c("precision", "n")))
  }
  precision_split$model <- alg_names[[model]]
  precision2 <- rbind(precision2, precision_split)
}


# Standardabweichung und durchschnitt der 10 kfolds berechnen.
precision2 <- precision2 %>%
  group_by(model, n) %>%
  summarise(
    mean_prec = mean(precision),
    sd_prec = sd(precision)
  )
```


```{r,fig.width=14,fig.height=12}
# sample 2
# Basis Modell
top_n_movies2 <- user_item_r2 %>%
  group_by(movieId) %>%
  summarise(mean_rating = mean(z_rating)) %>%
  arrange(desc(mean_rating)) %>%
  head(30) %>%
  rename(name = movieId)
class(top_n_movies2$name) <- "character"

top_n <- top_n_movies2$name

prec_best_mean2 <- as.data.frame({})

for (n in topn) {
  prec_fold <- list()
  for (n_split in 1:10) {
    pred_movies <- getData(scheme2, "unknown", run = n_split)
    pred_movies <- as.data.frame(as(pred_movies, "matrix"))
    pred_movies$username <- rownames(pred_movies)
    pred_movies_long <- pivot_longer(pred_movies, cols = -c(username), values_drop_na = TRUE)

    prec <- nrow(pred_movies_long %>%
      filter(name %in% top_n[1:n])) / n / 40

    prec_fold <- append(prec_fold, prec)
  }
  prec_best_mean2 <- rbind(prec_best_mean2, as.data.frame(list(mean(unlist(prec_fold)), sd(unlist(prec_fold)), n, "pre_best_mean"), col.names = c("mean_prec", "sd_prec", "n", "model")))
}

precision2 <- rbind(precision2, prec_best_mean)


# plot
p2 <- list()
i <- 1
for (n_value in topn) {
  pre_plot <- filter(precision2, n == n_value)
  plot <- (ggplot(pre_plot, aes(x = model, y = mean_prec)) +
    geom_point() +
    geom_errorbar(aes(ymin = mean_prec - sd_prec, ymax = mean_prec + sd_prec),
      width = .2,
      position = position_dodge(.9)
    ) +
    ylim(0, 0.2) +
    labs(
      title = paste("Precision von Top -", n_value, " Listen"),
      x = "Modell",
      y = "Precision",
      caption = "Datensatz 2"
    ))
  p2[[i]] <- plot

  i <- i + 1
}
do.call(grid.arrange, c(p2, ncol = 2))
```

```{r}
ggplot(precision2, aes(x = n, y = mean_prec,color = model)) +
  geom_point() +
    ylim(0, 0.2) +
    labs(
      title = paste("Precision von Top - n Listen"),
      x = "Länge Top-N Liste",
      y = "Precision",
      color = "Modelle",
      caption = "Datensatz 2"
    )
```

**Beobachtung Precision:**

* UBCF hat die höchste maximale Precision von 20 % und hat die grösste Standardabweichung bei den unterschiedlichen Folds.
* Die Precision sinkt bei der steigender Länge der Top-N Liste.
* Die Precision des Top Movie Recommenders verändert sich nicht je nach Länge der Top-N Liste.
* Binary IBCF hat vorallem bei steigender Länge der Top-N Liste die höchste durschnittliche Precision. 
* Im Verlauf der Top-N Listen steigt die Precision von SVD, Top-Movie Recommender und UBCF.

**Interpretation Precision:**

Die Modelle sind was die Precision anbelangt nicht alzu gut. Der Top-Movie Recommender ist gleich gut oder besser wie alle anderen Modelle. Eine mögliche Erklärung ist, dass das verwenden der standardisierten Ratings pro User nicht sinnvoll waren. Zudem ist die steigende Precision bei SVD und UBCF irritierend. Wir sehen keinen Grund, wieso diese steigend sein sollten. Bei einer weiteren Analyse müsste dies noch genauer untersucht werden.

### Coverage Novelty berechnen

Coverage und Novelty von den unterschiedlichen Modellen berechnen.

```{r}
# sample 1
# novelty base model
top_n_movies1 <- top_n_movies1 %>% rename(movieId = name)
class(top_n_movies1$movieId) <- "integer"
pred_with_log <- left_join(top_n_movies1,
  log_popularity1,
  by = "movieId"
)

novelty_best_mean_model <- c()
for (n_value in topn) {
  novelty_best_mean_model <- append(novelty_best_mean_model, (-sum(head(pred_with_log, n_value)$log_pop) / 1 / n_value))
}


# coverage base model
coverage_best_mean_model <- c()
for (n_value in topn) {
  coverage_best_mean_model <- append(coverage_best_mean_model, n_value / 700)
}

# coverage & novelty recommender
novelty_values <- as.data.frame({})
coverage_values <- as.data.frame({})
i <- 1
for (top in topn) {
  for (algorithm in algorithms) {
    nov_fold <- list()
    cov_fold <- list()
    for (n_split in 1:10) {
      trained_model <- Recommender(getData(scheme1, "train", run = n_split), algorithm$name,
        param = algorithm$param
      )

      # predict model
      pred_model <- predict(trained_model, getData(scheme1, "unknown", run = n_split), type = "topNList", n = top)
      nov_fold <- append(nov_fold, novelty(pred_model, log_popularity1, top, 40))
      cov_fold <- append(cov_fold, coverage(pred_model))
    }
    novelty_values <- rbind(novelty_values, as.data.frame(list(mean(unlist(nov_fold)), sd(unlist(nov_fold)), top, alg_names[[i %% 5 + 1]]), col.names = c("mean_novel", "sd_novel", "n", "model")))
    coverage_values <- rbind(coverage_values, as.data.frame(list(mean(unlist(cov_fold)), sd(unlist(cov_fold)), top, alg_names[[i %% 5 + 1]]), col.names = c("mean_cov", "sd_cov", "n", "model")))
    i <- i + 1
  }
}
```


```{r}
# sample 2
# novelty base model
top_n_movies2 <- top_n_movies2 %>% rename(movieId = name)
class(top_n_movies2$movieId) <- "integer"
pred_with_log <- left_join(top_n_movies2,
  log_popularity2,
  by = "movieId"
)

novelty_best_mean_model2 <- c()
for (n_value in topn) {
  novelty_best_mean_model2 <- append(novelty_best_mean_model2, (-sum(head(pred_with_log, n_value)$log_pop) / 1 / n_value))
}

# coverage base model
coverage_best_mean_model2 <- c()
for (n_value in topn) {
  coverage_best_mean_model2 <- append(coverage_best_mean_model2, n_value / 700)
}

# coverage & novelty recommender
novelty_values2 <- as.data.frame({})
coverage_values2 <- as.data.frame({})
i <- 1
for (top in topn) {
  for (algorithm in algorithms) {
    nov_fold <- list()
    cov_fold <- list()
    for (n_split in 1:10) {
      trained_model <- Recommender(getData(scheme2, "train", run = n_split), algorithm$name,
        param = algorithm$param
      )

      # predict model
      pred_model <- predict(trained_model, getData(scheme2, "unknown", run = n_split), type = "topNList", n = top)
      nov_fold <- append(nov_fold, novelty(pred_model, log_popularity2, top, 40))
      cov_fold <- append(cov_fold, coverage(pred_model))
    }
    novelty_values2 <- rbind(novelty_values2, as.data.frame(list(mean(unlist(nov_fold)), sd(unlist(nov_fold)), top, alg_names[[i %% 5 + 1]]), col.names = c("mean_novel", "sd_novel", "n", "model")))
    coverage_values2 <- rbind(coverage_values2, as.data.frame(list(mean(unlist(cov_fold)), sd(unlist(cov_fold)), top, alg_names[[i %% 5 + 1]]), col.names = c("mean_cov", "sd_cov", "n", "model")))
    i <- i + 1
  }
}
```




### Novelty visualisieren

```{r,fig.width=16,fig.height=12}
p <- list()
i <- 1
for (n_value in topn) {
  pre_plot <- filter(novelty_values, n == n_value)
  plot <- (ggplot(pre_plot, aes(x = model, y = mean_novel)) +
    geom_point() +
    geom_errorbar(aes(ymin = mean_novel - sd_novel, ymax = mean_novel + sd_novel),
      width = .2,
      position = position_dodge(.9)
    ) +
    labs(
      title = paste("Novelty von Top -", n_value, " Listen"),
      subtitle = paste("Novelty Top Movie: ", round(novelty_best_mean_model[[i]], 2)),
      x = "Modell",
      y = "Novelty",
      caption = "Datensatz 1"
    ))
  p[[i]] <- plot

  i <- i + 1
}

do.call(grid.arrange, c(p, ncol = 2))
```

```{r,fig.width=16,fig.height=12}
p2 <- list()
i <- 1
for (n_value in topn) {
  pre_plot <- filter(novelty_values2, n == n_value)
  plot <- (ggplot(pre_plot, aes(x = model, y = mean_novel)) +
    geom_point() +
    geom_errorbar(aes(ymin = mean_novel - sd_novel, ymax = mean_novel + sd_novel),
      width = .2,
      position = position_dodge(.9)
    ) +
    labs(
      title = paste("Novelty von Top -", n_value, " Listen"),
      subtitle = paste("Novelty Top Movie: ", round(novelty_best_mean_model2[[i]], 2)),
      x = "Modell",
      y = "Novelty",
      caption = "Datensatz 2"
    ))
  p2[[i]] <- plot

  i <- i + 1
}
do.call(grid.arrange, c(p2, ncol = 2))
```

**Beobachtung:**

Die Novelty von IBCF ist am höchsten und die von Binary IBCF am tiefsten. Die Novelty vom Top Movie Recommender ist ähnlich hoch, wie von den sonstigen Recommender.


**Interpretation:**

Da die Filme im Top-Movie Recommender nicht alzu häufig im Datensatz vorkommen, ist die Novelty ähnlich hoch. Ansonsten fällt vorallem der Binary IBCF Recommender ab.

### Coverage Visualisieren

```{r,fig.width=16,fig.height=12}
# sample 1
# plot
p <- list()
i <- 1
for (n_value in topn) {
  pre_plot <- filter(coverage_values, n == n_value)
  plot <- (ggplot(pre_plot, aes(x = model, y = mean_cov)) +
    geom_point() +
    geom_errorbar(aes(ymin = mean_cov - sd_cov, ymax = mean_cov + sd_cov),
      width = .2,
      position = position_dodge(.9)
    ) +
    labs(
      title = paste("Coverage von Top -", n_value, " Listen"),
      subtitle = paste("Coverage Top Movie: ", round(coverage_best_mean_model[[i]], 2)),
      x = "Modell",
      y = "Coverage",
      caption = "Datensatz 1"
    ))
  p[[i]] <- plot

  i <- i + 1
}
do.call(grid.arrange, c(p, ncol = 2))
```

```{r,fig.width=16,fig.height=12}
# sample 2
# plot
p2 <- list()
i <- 1
for (n_value in topn) {
  pre_plot <- filter(coverage_values2, n == n_value)
  plot <- (ggplot(pre_plot, aes(x = model, y = mean_cov)) +
    geom_point() +
    geom_errorbar(aes(ymin = mean_cov - sd_cov, ymax = mean_cov + sd_cov),
      width = .2,
      position = position_dodge(.9)
    ) +
    labs(
      title = paste("Coverage von Top -", n_value, " Listen"),
      subtitle = paste("Coverage Top Movie: ", round(coverage_best_mean_model[[i]], 2)),
      x = "Modell",
      y = "Coverage",
      caption = "Datensatz 2"
    ))
  p2[[i]] <- plot

  i <- i + 1
}
do.call(grid.arrange, c(p2, ncol = 2))
```

**Beobachtung:**

Der Top Movie Recommender hat die tiefste Coverage. Ansonsten hat SVD und UBCF eine tiefe Coverage und Binary UBCF hat die höchste Coverage.

**Interpretation:**

Der Top-Movie Recommender deckt nur sehr wenige Filme ab, da jedesmal die gleichen Filme empfohlen werden.


### Auswahl des Recommenders

Die IBCF Recommender ist der Kompromiss zwischen Novelty, Coverage und Precision. Der IBCF hebt sich vor allem mit eine guten Coverage und Novelty von den anderen Recommender ab.

## Hyperparameter Trainieren
Das IBCF Modell hat zwei Hyperparameter. Die Anzahl Nachbarn und ob die NA auf 0 gesetzt werden. Die Hyperparameter werden mit der Top-N Liste von 10 und der Precision als Metrik gemacht. Die Precision wurde gewählt, weil der ausgewählte Recommender bei dieser Metrik am meisten abfallt. Es wäre auch möglich eine Kombination von Metriken zu wählen.

```{r, results='hide'}
# sample 1
best_precision <- 0
for (neighbours in (1:20) * 2) {
  for (na_as_zero in c(TRUE, FALSE)) {
    algorithm_test <- list("item-based CF" = list(name = "IBCF", param = list(method = "Cosine", k = neighbours, na_as_zero = na_as_zero)))
    results_best_model <- evaluate(scheme1, algorithm_test, type = "topNList", n = 10)
    mean_precision <- avg(results_best_model)[[1]][[6]]
    if (best_precision < mean_precision) {
      best_precision <- mean_precision
      best_algorithm <- algorithm_test
    }
  }
}

algorithm_k1 <- best_algorithm$`item-based CF`$param$k
algorithm_na_as_zero1 <- best_algorithm$`item-based CF`$param$na_as_zero
```

```{r}
# sample 1
nov_fold <- list()
cov_fold <- list()
for (n_split in 1:10) {
  trained_model <- Recommender(getData(scheme1, "train", run = n_split), best_algorithm[[1]]$name,
    param = best_algorithm[[1]]$param
  )

  # predict model
  pred_model <- predict(trained_model, getData(scheme1, "unknown", run = n_split), type = "topNList", n = 10)
  nov_fold <- append(nov_fold, novelty(pred_model, log_popularity1, top, 40))
  cov_fold <- append(cov_fold, coverage(pred_model))
}

nov_fold1 <- round(mean(unlist(nov_fold)), 2)
cov_fold1 <- round(mean(unlist(cov_fold)), 2)
best_precision <- round(best_precision, 2)
```


```{r,, results='hide'}
# sample 2
best_precision2 <- 0
for (neighbours in (1:20) * 2) {
  for (na_as_zero in c(TRUE, FALSE)) {
    algorithm_test <- list("item-based CF" = list(name = "IBCF", param = list(method = "Cosine", k = neighbours, na_as_zero = na_as_zero)))
    results_best_model <- evaluate(scheme2, algorithm_test, type = "topNList", n = 10)
    mean_precision <- avg(results_best_model)[[1]][[6]]
    if (best_precision2 < mean_precision) {
      best_precision2 <- mean_precision
      best_algorithm2 <- algorithm_test
    }
  }
}
algorithm_k2 <- best_algorithm2$`item-based CF`$param$k
algorithm_na_as_zero2 <- best_algorithm2$`item-based CF`$param$na_as_zero
```

```{r}
# sample 2
nov_fold <- list()
cov_fold <- list()
for (n_split in 1:10) {
  trained_model <- Recommender(getData(scheme2, "train", run = n_split), best_algorithm2[[1]]$name,
    param = best_algorithm2[[1]]$param
  )

  # predict model
  pred_model <- predict(trained_model, getData(scheme2, "unknown", run = n_split), type = "topNList", n = 30)
  nov_fold <- append(nov_fold, novelty(pred_model, log_popularity2, top, 40))
  cov_fold <- append(cov_fold, coverage(pred_model))
}
```

```{r}
mean_nov2 <- round(mean(unlist(nov_fold)), 2)
mean_cov2 <- round(mean(unlist(cov_fold)), 2)
best_precision2 <- round(best_precision2, 2)
```

**Beschreibung:**
Beim Datensatz 1 erzielt der beste Algorithmus basierend auf der Precision, eine Precision von `r best_precision` eine Novelty von `r nov_fold1` und eine Coverage von `r cov_fold1`.
Dabei wurde der IBCF Recommender mit der Cosinus Ähnlichkeit, `r algorithm_k1` Nachbarn und na_as_zero = `r algorithm_na_as_zero1`.

Beim Datensatz 2 erzielt der beste Algorithmus basierend auf der Precision, eine Precision von `r best_precision2` eine Novelty von `r mean_nov2` und eine Coverage von `r mean_cov2`.
Dabei wurde der IBCF Recommender mit der Cosinus Ähnlichkeit, `r algorithm_k2` Nachbarn und na_as_zero = `r algorithm_na_as_zero2`.

**Interpretation:**
Beim Datensatz 1 konnte eine im Vergleich zu den anderen Modellen eine gute Precision und eine einigermassen gute Coverage erreicht werden. Dafür erreicht das Modell eine schlechte Novelty. Das bedeutet es findet hauptsächlich Filme heraus die schon öfters im Datensatz vorkommen. Dies könnte an der niedrigen Anzahl Nachbarn liegen.

Beim Datensatz 2 ist die Coverage und Novelty hoch. Die Precision ist im Vergleich mittelmässig.

# Implementierung Top-N Monitor

## Fixieren von Testkunden

Die Testkunden wurden Random ausgewählt, da durch die Vorauswahl der reduzierten Datensätze, spezifisch Kunden und Filme mit vielen Ratings ausgewählt werden. Wenn die Testkunden nochmals spezifischer ausgewählt werden würden, wären die Testkunden noch weniger Repräsentativ.

```{r}
# sample 1
set.seed(10)
user_sample1 <- sample(rownames(m_user_item_r1), 20)
```

```{r}
# sample 2
set.seed(21)
user_sample2 <- sample(rownames(m_user_item_r2), 20)
```

## Bestimme den Anteil der Top-N Empfehlung nach Genres pro Kunde

Es wurde ein IBCF-Recommender, ein UBCF-Recommender, ein SVD-Recommender mit 10 Singulärwerten und ein SVD-Recommender mit 20 Singulärwerten.
\
**Hypothese:** Wir gehen davon aus, dass die Genres pro Kunde der beiden SVD-Recommender nahe beieinander sind. Auch die beiden Modelle UBCF und IBCF sollten ähnliche Genres pro Kunde ausgeben.

```{r}
# sample 1
train_data <- as.matrix(m_user_item_r1[!(rownames(m_user_item_r1) %in% user_sample1), ])
test_data <- as.matrix(m_user_item_r1[(rownames(m_user_item_r1) %in% user_sample1), ])
```

```{r}
# sample 2
train_data2 <- as.matrix(m_user_item_r2[!(rownames(m_user_item_r2) %in% user_sample2), ])
test_data2 <- as.matrix(m_user_item_r2[(rownames(m_user_item_r2) %in% user_sample2), ])
```

```{r}
# sample 1
# calc Top-N IBCF
IBCF1 <- Recommender(as(train_data, "realRatingMatrix"), "IBCF",
  param = list(method = "cosine", k = 30, na_as_zero = TRUE)
)
pIBCF1 <- predict(IBCF1, as(test_data, "realRatingMatrix"), type = "topNList", n = 15)

# calc Top-N UBCF
UBCF1 <- Recommender(as(train_data, "realRatingMatrix"), "UBCF",
  param = list(method = "cosine", nn = 30)
)
pUBCF1 <- predict(UBCF1, as(test_data, "realRatingMatrix"), type = "topNList", n = 15)

# calc Top-N SVD k = 10
SVDk_10_1 <- Recommender(as(train_data, "realRatingMatrix"), "SVD",
  param = list(k = 10)
)
PSVDk_10_1 <- predict(SVDk_10_1, as(test_data, "realRatingMatrix"), type = "topNList", n = 15)

# calc Top-N SVD k = 20
SVDk_20_1 <- Recommender(as(train_data, "realRatingMatrix"), "SVD",
  param = list(k = 20)
)
PSVDk_20_1 <- predict(SVDk_20_1, as(test_data, "realRatingMatrix"), type = "topNList", n = 15)
```

```{r}
# sample 2
# calc Top-N IBCF
IBCF2 <- Recommender(as(train_data2, "realRatingMatrix"), "IBCF",
  param = list(method = "cosine", k = 30, na_as_zero = TRUE)
)
pIBCF2 <- predict(IBCF2, as(test_data2, "realRatingMatrix"), type = "topNList", n = 15)

# calc Top-N UBCF
UBCF2 <- Recommender(as(train_data2, "realRatingMatrix"), "UBCF",
  param = list(method = "cosine", nn = 30)
)
pUBCF2 <- predict(UBCF2, as(test_data2, "realRatingMatrix"), type = "topNList", n = 15)

# calc Top-N SVD k = 10
SVDk_10_2 <- Recommender(as(train_data2, "realRatingMatrix"), "SVD",
  param = list(k = 10)
)
PSVDk_10_2 <- predict(SVDk_10_2, as(test_data2, "realRatingMatrix"), type = "topNList", n = 15)

# calc Top-N SVD k = 20
SVDk_20_2 <- Recommender(as(train_data2, "realRatingMatrix"), "SVD",
  param = list(k = 20)
)
PSVDk_20_2 <- predict(SVDk_20_2, as(test_data2, "realRatingMatrix"), type = "topNList", n = 15)
```


```{r}
get_proportion <- function(pred_recommender, test_data, name_recommender, genre_data) {
  predTopN <- as(pred_recommender, "matrix")

  rownames(predTopN) <- rownames(test_data)
  predTopN <- predTopN[, order(as.integer(colnames(predTopN)))]
  predTopN[is.na(predTopN) == FALSE] <- 1
  predTopN[is.na(predTopN)] <- 0

  dummies <- genre_data %>%
    filter(movieId %in% colnames(predTopN)) %>%
    dummy_columns(select_columns = "genres") %>%
    select(-c("genres", "title"))
  dummies <- aggregate(dummies, by = list(name = dummies$movieId), max)
  rownames(dummies) <- dummies$movieId
  dummies <- as.matrix(select(dummies, -c("movieId", "name")))



  genres_proportion <- as.data.frame(predTopN %*% dummies / 15)
  genres_proportion$userId <- rownames(genres_proportion)
  genres_proportion <- genres_proportion %>%
    pivot_longer(!userId, names_to = "genres", values_to = "prop")
  genres_proportion$genres <- str_replace_all(genres_proportion$genres, "genres_", "")
  genres_proportion$Top_N <- name_recommender

  return(genres_proportion)
}
```
```{r}
# sample 1
IBCF_prop <- get_proportion(pIBCF1, test_data, "IBCF", genres_sep1)
UBCF_prop <- get_proportion(pUBCF1, test_data, "UBCF", genres_sep1)
SVD_k_10_prop <- get_proportion(PSVDk_10_1, test_data, "SVD_k_10", genres_sep1)
SVD_k_20_prop <- get_proportion(PSVDk_20_1, test_data, "SVD_k_20", genres_sep1)

recommender_prop <- rbind(IBCF_prop, UBCF_prop) %>%
  rbind(SVD_k_10_prop) %>%
  rbind(SVD_k_20_prop)
```

```{r,fig.width=11,fig.height=14}
# sample 1
ggplot(recommender_prop, aes(x = prop, y = genres, color = Top_N)) +
  geom_jitter(width = 0.01, height = 0) +
  facet_wrap(userId ~ ., ncol = 5) +
  labs(
    title = "Anteil Genres",
    subtitle = "Top-Empfehlungen pro User",
    x = "Anteil Genres",
    y = "Genres",
    caption = "Datensatz 1"
  ) +
  theme_classic()
```

```{r}
# sample 2
IBCF_prop <- get_proportion(pIBCF2, test_data2, "IBCF", genres_sep2)
UBCF_prop <- get_proportion(pUBCF2, test_data2, "UBCF", genres_sep2)
SVD_k_10_prop <- get_proportion(PSVDk_10_2, test_data2, "SVD_k_10", genres_sep2)
SVD_k_20_prop <- get_proportion(PSVDk_20_2, test_data2, "SVD_k_20", genres_sep2)

recommender_prop2 <- rbind(IBCF_prop, UBCF_prop) %>%
  rbind(SVD_k_10_prop) %>%
  rbind(SVD_k_20_prop)
```

```{r,fig.width=11,fig.height=14}
# sample 2
ggplot(recommender_prop2, aes(x = prop, y = genres, color = Top_N)) +
  geom_jitter(width = 0.01, height = 0) +
  facet_wrap(userId ~ .) +
  labs(
    title = "Anteil Genres",
    subtitle = "Top-Empfehlungen pro User",
    x = "Anteil Genres",
    y = "Genres",
    caption = "Datensatz 2"
  ) +
  theme_classic()
```

**Beschreibung:** Die beiden SVD- Recommender sind nahe beieinander. Die anderen beiden Recommender habe keine Ähnlichkeiten zueinander. Zum Beispiel beim User 448 im ersten Datensatz erkennt man deutlich, dass der IBCF- Recommender bei diversen Genres weit weg von den anderen Algorithm ist.

**Interpretation:**Der erste Teil von der Hypothese kann bestätigt werden. Der zweite Teil mit den Ähnlichkeiten zwischen UBCF und IBCF wird abgelehnt. 


## Bestimme pro Kunde den Anteil nach Genres seiner Top-Filme

**Hypothese:** Wir gehen davon aus, dass die meisten User Filme von maximal 5 Genres bewertet und somit angesehen haben.

```{r}
get_proportion_user <- function(ratings_data, genres_data, test_data) {
  top_n_user <- ratings_data %>%
    filter(userId %in% rownames(test_data)) %>%
    arrange(desc(z_rating)) %>%
    group_by(userId) %>%
    slice(1:15) %>%
    ungroup() %>%
    select(c(userId, movieId, z_rating)) %>%
    pivot_wider(names_from = movieId, values_from = z_rating)

  row_top_n <- top_n_user$userId
  top_n_user <- as.matrix(select(top_n_user, -c(userId)))
  rownames(top_n_user) <- row_top_n
  top_n_user <- top_n_user[, order(as.integer(colnames(top_n_user)))]

  top_n_user[is.na(top_n_user) == FALSE] <- 1
  top_n_user[is.na(top_n_user)] <- 0
  
  dummies <- genres_data %>%
    filter(movieId %in% colnames(top_n_user)) %>%
    dummy_columns(select_columns = "genres") %>%
    select(-c("genres", "title"))
  dummies <- aggregate(dummies, by = list(name = dummies$movieId), max)
  rownames(dummies) <- dummies$movieId
  dummies <- as.matrix(select(dummies, -c("movieId", "name")))


  genres_proportion <- as.data.frame(top_n_user %*% dummies / 15)
  genres_proportion$userId <- rownames(genres_proportion)
  genres_proportion <- genres_proportion %>%
    pivot_longer(!userId, names_to = "genres", values_to = "prop")
  genres_proportion$genres <- str_replace_all(genres_proportion$genres, "genres_", "")
  genres_proportion$Top_N <- "User"
  return(genres_proportion)
}
```

```{r,fig.width=11,fig.height=14}
# sample 1
top_n_user1 <- get_proportion_user(norm_ratings1, genres_sep1, test_data)

ggplot(top_n_user1, aes(x = prop, y = genres)) +
  geom_point() +
  facet_wrap(userId ~ .) +
  labs(
    title = "Anteil Genres",
    subtitle = "Top-Filme pro User",
    x = "Anteil Genres",
    y = "Genres",
    caption = "Datensatz 1"
  ) +
  theme_classic()
```

```{r,fig.width=11,fig.height=14}
# sample 2
top_n_user2 <- get_proportion_user(norm_ratings2, genres_sep2, test_data2)

ggplot(top_n_user2, aes(x = prop, y = genres)) +
  geom_point() +
  facet_wrap(userId ~ .) +
  labs(
    title = "Anteil Genres",
    subtitle = "Top-Filme pro User",
    x = "Anteil Genres",
    y = "Genres",
    caption = "Datensatz 2"
  ) +
  theme_classic()
```

**Beschreibung:** Die meisten User haben diverse Genres, welche sie nicht interessiert. Es gibt aber auch Kunden wie der User 96 im Datensatz 2, die Filme bei sehr unterschiedlichen Genres angesehen haben.

**Interpretation:** Die Hypothese kann unter vorbehalt bestätigt werden. Die meisten Nutzer konzentrieren sich auf ein paar wenige Genres.

## Vergleiche pro Kunde Top-Empfehlungen vs Top-Filme nach Genres

**Hypothese:** Das Profil mit den Top-Empfehlungen liegt zwischen den unterschiedlichen Genres.

```{r,fig.width=11,fig.height=14}
# sample 1
colors <- c("#7d3c98", "#85c1e9", "#45b39d", "#f39c12", "#000000")
recommender_user_prop <- rbind(recommender_prop, top_n_user1)
ggplot(recommender_user_prop, aes(x = prop, y = genres, color = Top_N)) +
  geom_jitter(width = 0.01, height = 0, alpha = 1) +
  scale_color_manual(values = colors) +
  facet_wrap(userId ~ .) +
  labs(
    title = "Anteil Genres pro User",
    subtitle = "Top-Empfehlungen vs top-Filme",
    x = "Anteil Genres",
    y = "Genres",
    caption = "Datensatz 1"
  ) +
  theme_classic()
```

```{r,fig.width=11,fig.height=14}
# sample 2
recommender_user_prop2 <- rbind(recommender_prop2, top_n_user2)
ggplot(recommender_user_prop2, aes(x = prop, y = genres, color = Top_N)) +
  geom_jitter(width = 0.01, height = 0, alpha = 1) +
  scale_color_manual(values = colors) +
  facet_wrap(userId ~ ., ncol = 5) +
  labs(
    title = "Anteil Genres pro User",
    subtitle = "Top-Empfehlungen vs top-Filme",
    x = "Anteil Genres",
    y = "Genres",
    caption = "Datensatz 2"
  ) +
  theme_classic()
```

**Beschreibung:** Das Profil der Top- Empfehlungen liegt nicht immer zwischen den mit den Recommender Systemen berechneten Profilen. Im grossen und ganzen liegen die Profile nahe beieinander.

**Interpretation:**Die Hypothese kann zwar abgelehnt werden. Trotzdem funktionieren die Recommender nicht schlecht. Sie sollten auch nicht unbedingt um das Top- Empfehlungen Profil streuen. Dann würden wahrscheinlich zuviele Blockbuster vorhergesagt werden.

## Definiere eine Qualitätsmetrik für Top-N Listen und teste sie.

**Erklärung der Auswahl der Qualitätsmetrik:**

Um den Anteil von Genres zu vergleichen gibt es unterschiedliche Möglichkeiten. Zum einem könnte man, wieder ein Ähnlichkeitsmass verwenden. Es ist es auch möglich ein Distanzmass zu verwenden. Eine Herausforderung besteht darin, dass jeder Filme diverse Genres haben kann. Das führt zu einem Rauschen in den Daten. Ein Action Liebhaber könnte einen Action- Film geschaut haben, welcher auch dem Genre Romantik zugewiesen ist. Wenn er 10 Action Filme geschaut hat, sollte der Anteil von Romantik keinen Einfluss haben.

Das führt zu einem Problem mit beiden Metriken. Die Genres die der User sehr stark mag, werden nicht zusätzlich Gewichtet. Darum verwenden wir in diesem Abschnitt die L2- Norm von den 3 Genres mit dem grössten Anteil.

```{r}
# Sample 1
top3_1 <- recommender_user_prop %>%
  pivot_wider(names_from = Top_N, values_from = prop) %>%
  arrange(desc(User)) %>%
  group_by(userId) %>%
  slice(1:3)


recommender <- list("IBCF", "UBCF", "SVD_k_10", "SVD_k_20")
top3_metrik <- list()
for (rec in recommender) {
  top3_metrik <- append(top3_metrik, norm(top3_1$User - top3_1[[rec]], type = "2"))
}
```

```{r}
# Sample 1
do.call(rbind, Map(data.frame, Top3Metrik = top3_metrik, Recommender = recommender)) %>%
  ggplot(., aes(Recommender, Top3Metrik)) +
  geom_bar(stat = "identity") +
  labs(
    title = "Qualitaetsmetrik der Recommender",
    x = "Recommender",
    y = "Qualitaetsmetrik",
    caption = "Datensatz 1"
  ) +
  theme_classic()
```

```{r}
# Sample 2
top3_2 <- recommender_user_prop2 %>%
  pivot_wider(names_from = Top_N, values_from = prop) %>%
  arrange(desc(User)) %>%
  group_by(userId) %>%
  slice(1:3)


recommender <- list("IBCF", "UBCF", "SVD_k_10", "SVD_k_20")
top3_metrik <- list()
for (rec in recommender) {
  top3_metrik <- append(top3_metrik, norm(top3_2$User - top3_2[[rec]], type = "2"))
}
```

```{r}
# Sample 2
do.call(rbind, Map(data.frame, Top3Metrik = top3_metrik, Recommender = recommender)) %>%
  ggplot(., aes(Recommender, Top3Metrik)) +
  geom_bar(stat = "identity") +
  labs(
    title = "Qualitaetsmetrik der Recommender",
    x = "Recommender",
    y = "Qualitaetsmetrik",
    caption = "Datensatz 2"
  ) +
  theme_classic()
```

**Beschreibung:**
Der IBCF-Recommender hat in beiden Daten den tiefsten Wert. Die SVD-Recommender mit den 10 und 20 Singulärwerten haben die höchsten Werte.

**Interpretation:**
Was diese Metrik anbelangt erziehlt der IBCF Recommender die besten Resultate. Eine absolute Aussage kann nicht getroffen werden, da es nur mit den 20 Nutzern ausgewertet wurde.

# Fazit

Es war spannend zu sehen wie die unterschiedlichen Recommender Systeme aufgebaut sind. Für uns war die grosse Erkentnisse, dass die Evaluierung von Recommender Systemen nicht einfach ist. Um einen optimalen Recommender zu finden müsste man stärker mit Rückmeldungen von Usern arbeiten. Zudem war es wahrscheinlich vom Vorgehen nicht optimal, die Ratings zu standardisieren.
